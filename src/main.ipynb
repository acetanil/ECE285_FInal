{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.applications.resnet50 as resnet50 \n",
    "import tensorflow.keras.applications.vgg19 as vgg19\n",
    "import tensorflow.keras.applications.densenet as densenet\n",
    "import tensorflow.keras.applications.inception_v3 as inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.preprocessing.image as image\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from os.path import join\n",
    "\n",
    "prefix = './Stanford_Dog_Breed'\n",
    "imgfix = 'Images'\n",
    "anofix = 'Annotation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12000 images belonging to 120 classes.\n",
      "Found 8580 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "ImgTrainGen = image.ImageDataGenerator(preprocessing_function=inception_v3.preprocess_input, \n",
    "                                  width_shift_range=0.2, \n",
    "                                  height_shift_range=0.2, \n",
    "                                  shear_range=0.2, \n",
    "                                  zoom_range=0.2, \n",
    "                                  horizontal_flip=True, \n",
    "#                                   validation_split=1/6.\n",
    "                                      )\n",
    "\n",
    "Train = ImgTrainGen.flow_from_directory(join(prefix, imgfix, 'Train'), \n",
    "                                   target_size=(224, 224), \n",
    "                                   class_mode='sparse', \n",
    "                                   batch_size=32, \n",
    "                                   shuffle=True, \n",
    "                                   seed=None, \n",
    "                                   subset='training', \n",
    "                                   interpolation='nearest')\n",
    "\n",
    "# Valid = ImgTrainGen.flow_from_directory(join(prefix, imgfix, 'Train'), \n",
    "#                                    target_size=(224, 224), \n",
    "#                                    class_mode='sparse', \n",
    "#                                    batch_size=32, \n",
    "#                                    shuffle=True, \n",
    "#                                    seed=None, \n",
    "#                                    subset='validation', \n",
    "#                                    interpolation='nearest')\n",
    "\n",
    "\n",
    "ImgTestGen = image.ImageDataGenerator(preprocessing_function=inception_v3.preprocess_input, validation_split=1/6.)\n",
    "\n",
    "Test  = ImgTestGen.flow_from_directory(join(prefix, imgfix, 'Test' ), \n",
    "                                       target_size=(224, 224), \n",
    "                                       class_mode='sparse', \n",
    "                                       batch_size=32, \n",
    "                                       shuffle=False, \n",
    "                                       interpolation='nearest')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiTaskDataGen(ImgGen):\n",
    "    while True:\n",
    "        X = ImgGen.next()\n",
    "        yield [X[0], X[1]], [X[1], X[1], X[1]]\n",
    "        \n",
    "TrainData = MultiTaskDataGen(Train)\n",
    "# ValidData = MultiTaskDataGen(Valid)\n",
    "TestData  = MultiTaskDataGen(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def gen_path(idx):\n",
    "    try:    path = join(prefix, imgfix, ftest[idx][0][0])\n",
    "    except: path = join(prefix, imgfix, ftest[-1][0][0])      \n",
    "    return  path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def gen_labl(idx):\n",
    "    try:    labl = ltest[idx][0]\n",
    "    except: labl = ltest[-1][0]\n",
    "    return  labl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_imgs(idx):\n",
    "    img = image.load_img(gen_path(idx), target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XTrain = []\n",
    "for idx in range(ntrain):\n",
    "    path = join(prefix, imgfix, ftrain[idx][0][0])\n",
    "    try:\n",
    "        img = image.load_img(path, target_size=(224, 224))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        XTrain.append(resnet50.preprocess_input(x))\n",
    "    except:\n",
    "        print('Error at processing the', idx, 'images.')\n",
    "\n",
    "XTrain = np.vstack(XTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XTest = []\n",
    "for idx in range(ntest):\n",
    "    path = gen_path(idx)\n",
    "    try:\n",
    "        img = image.load_img(path, target_size=(224, 224))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        XTest.append(resnet50.preprocess_input(x))\n",
    "    except:\n",
    "        print('Error at processing the', idx, 'images.')\n",
    "        \n",
    "XTest = np.vstack(XTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify using ImageNet Classes with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_naive = resnet50.ResNet50(weights='imagenet')\n",
    "# YTest = resnet50_naive.predict(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_naive.compile('SGD', loss=losses.categorical_crossentropy, metrics={'output_a': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTest_ = resnet50_naive.evaluate_generator(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTest_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.total_batches_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print XTest[0:1000, :, :, :].shape\n",
    "print YTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YTest = resnet50.decode_predictions(YTest, top=3)[0]\n",
    "\n",
    "correct, top = 0, 1\n",
    "\n",
    "for idx in range(ntest):\n",
    "    pred = resnet50.decode_predictions(YTest[idx:idx+1,:], top=top)[0]\n",
    "    for k in range(top):\n",
    "        if pred[k][1] not in DogBreed:\n",
    "            continue\n",
    "        labl = DogBreed[pred[k][1]]\n",
    "        if labl == gen_labl(idx):\n",
    "            correct += 1\n",
    "            break\n",
    "\n",
    "\n",
    "print(correct)\n",
    "print(correct / float(ntest))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_naive = inception_v3.InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model Plug with FC120 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs \n",
    "input_image = inceptionv3_naive.input\n",
    "\n",
    "# Intermediate Layers\n",
    "x = inceptionv3_naive.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "# x = layers.Dense(240, activation='relu')(x)\n",
    "x = layers.Dense(120, activation='softmax')(x)\n",
    "\n",
    "# Compile\n",
    "model = models.Model(inputs=input_image, outputs=x)\n",
    "for layer in inceptionv3_naive.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer='SGD', \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "class SaveModelandEval(callbacks.Callback):\n",
    "    prev_res = list()\n",
    "    def __init__(self):\n",
    "        self.prev_res = [0., 0.]\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch == 0: return\n",
    "        if epoch % 3 == 0:\n",
    "            res = model.evaluate_generator(Test, verbose=0, steps=None, workers=4, use_multiprocessing=True, max_queue_size=12)\n",
    "            print('\\n', res)\n",
    "            if res[1] > self.prev_res[1]:\n",
    "                models.save_model(model, 'inceptionv3_dfc120_%s_%s'%(epoch, int(res[1]*10000)) )\n",
    "                self.prev_res = res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/201\n",
      "80/80 [==============================] - 13s 157ms/step - loss: 4.8375 - acc: 0.0172\n",
      "Epoch 2/201\n",
      "80/80 [==============================] - 9s 109ms/step - loss: 4.5872 - acc: 0.0469\n",
      "Epoch 3/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 4.3625 - acc: 0.1047\n",
      "Epoch 4/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 4.1474 - acc: 0.1812\n",
      " [2.9085619246209418, 0.43228438228438226]\n",
      "80/80 [==============================] - 42s 527ms/step - loss: 4.1454 - acc: 0.1820\n",
      "Epoch 5/201\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 3.9823 - acc: 0.2223\n",
      "Epoch 6/201\n",
      "80/80 [==============================] - 10s 121ms/step - loss: 3.7589 - acc: 0.2887\n",
      "Epoch 7/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 3.5903 - acc: 0.3192\n",
      " [1.8556325831891218, 0.6206293706293706]\n",
      "80/80 [==============================] - 40s 494ms/step - loss: 3.5878 - acc: 0.3188\n",
      "Epoch 8/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 3.4401 - acc: 0.3543\n",
      "Epoch 9/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 3.3301 - acc: 0.3727\n",
      "Epoch 10/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 3.1660 - acc: 0.4062\n",
      " [1.3006715796766304, 0.6850815850815851]\n",
      "80/80 [==============================] - 40s 500ms/step - loss: 3.1628 - acc: 0.4070\n",
      "Epoch 11/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 2.9973 - acc: 0.4434\n",
      "Epoch 12/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 2.9108 - acc: 0.4430\n",
      "Epoch 13/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.8159 - acc: 0.4620\n",
      " [1.0169801892378392, 0.7226107226107226]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 2.8118 - acc: 0.4625\n",
      "Epoch 14/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 2.7571 - acc: 0.4625\n",
      "Epoch 15/201\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 2.6153 - acc: 0.4859\n",
      "Epoch 16/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.6062 - acc: 0.4810\n",
      " [0.8816534484669323, 0.7390442890442891]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 2.6045 - acc: 0.4809\n",
      "Epoch 17/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 2.4704 - acc: 0.5090\n",
      "Epoch 18/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 2.4209 - acc: 0.5086\n",
      "Epoch 19/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.4270 - acc: 0.5000\n",
      " [0.815492555504287, 0.7516317016317017]\n",
      "80/80 [==============================] - 40s 496ms/step - loss: 2.4281 - acc: 0.4992\n",
      "Epoch 20/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 2.3497 - acc: 0.5082\n",
      "Epoch 21/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 2.3223 - acc: 0.5172\n",
      "Epoch 22/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.2646 - acc: 0.5166\n",
      " [0.7725646443595559, 0.7614219114219114]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 2.2685 - acc: 0.5148\n",
      "Epoch 23/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 2.2396 - acc: 0.5273\n",
      "Epoch 24/201\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 2.2007 - acc: 0.5250\n",
      "Epoch 25/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.1524 - acc: 0.5352\n",
      " [0.7407016580911366, 0.7734265734265734]\n",
      "80/80 [==============================] - 39s 493ms/step - loss: 2.1580 - acc: 0.5340\n",
      "Epoch 26/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 2.0906 - acc: 0.5555\n",
      "Epoch 27/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 2.1159 - acc: 0.5410\n",
      "Epoch 28/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.0319 - acc: 0.5479\n",
      " [0.7391129634422868, 0.7744755244755245]\n",
      "80/80 [==============================] - 39s 485ms/step - loss: 2.0301 - acc: 0.5488\n",
      "Epoch 29/201\n",
      "80/80 [==============================] - 10s 128ms/step - loss: 2.0209 - acc: 0.5613\n",
      "Epoch 30/201\n",
      "80/80 [==============================] - 10s 119ms/step - loss: 2.0123 - acc: 0.5500\n",
      "Epoch 31/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.9948 - acc: 0.5542\n",
      " [0.7281995168263483, 0.7794871794871795]\n",
      "80/80 [==============================] - 40s 498ms/step - loss: 1.9972 - acc: 0.5531\n",
      "Epoch 32/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 2.0356 - acc: 0.5488\n",
      "Epoch 33/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.9461 - acc: 0.5551\n",
      "Epoch 34/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.9549 - acc: 0.5613\n",
      " [0.7299172739638884, 0.7793706293706294]\n",
      "80/80 [==============================] - 39s 484ms/step - loss: 1.9566 - acc: 0.5598\n",
      "Epoch 35/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.8990 - acc: 0.5680\n",
      "Epoch 36/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 1.9261 - acc: 0.5609\n",
      "Epoch 37/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.9016 - acc: 0.5593\n",
      " [0.7278011507829022, 0.7822843822843822]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 1.9043 - acc: 0.5590\n",
      "Epoch 38/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.8804 - acc: 0.5703\n",
      "Epoch 39/201\n",
      "80/80 [==============================] - 9s 118ms/step - loss: 1.8455 - acc: 0.5766\n",
      "Epoch 40/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.8754 - acc: 0.5585\n",
      " [0.7285193728728628, 0.7867132867132867]\n",
      "80/80 [==============================] - 39s 492ms/step - loss: 1.8716 - acc: 0.5602\n",
      "Epoch 41/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 1.8691 - acc: 0.5648\n",
      "Epoch 42/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.8449 - acc: 0.5730\n",
      "Epoch 43/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.8264 - acc: 0.5601\n",
      " [0.7251158460534811, 0.7857808857808858]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.8288 - acc: 0.5598\n",
      "Epoch 44/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.8270 - acc: 0.5805\n",
      "Epoch 45/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.8277 - acc: 0.5715\n",
      "Epoch 46/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.8214 - acc: 0.5700\n",
      " [0.7268220718821136, 0.7888111888111888]\n",
      "80/80 [==============================] - 39s 486ms/step - loss: 1.8220 - acc: 0.5691\n",
      "Epoch 47/201\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.7562 - acc: 0.5926\n",
      "Epoch 48/201\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 1.7491 - acc: 0.5883\n",
      "Epoch 49/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.7830 - acc: 0.5752\n",
      " [0.7236992035725014, 0.7952214452214452]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.7836 - acc: 0.5754\n",
      "Epoch 50/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.7637 - acc: 0.5898\n",
      "Epoch 51/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.7711 - acc: 0.5805\n",
      "Epoch 52/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.7500 - acc: 0.5811\n",
      " [0.7377185428036526, 0.7903263403263403]\n",
      "80/80 [==============================] - 39s 492ms/step - loss: 1.7493 - acc: 0.5820\n",
      "Epoch 53/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.7105 - acc: 0.5840\n",
      "Epoch 54/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.7018 - acc: 0.5836\n",
      "Epoch 55/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.6883 - acc: 0.6005\n",
      " [0.7384107571336199, 0.7921911421911422]\n",
      "80/80 [==============================] - 38s 481ms/step - loss: 1.6882 - acc: 0.6012\n",
      "Epoch 56/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.7376 - acc: 0.5824\n",
      "Epoch 57/201\n",
      "80/80 [==============================] - 10s 130ms/step - loss: 1.7016 - acc: 0.5852\n",
      "Epoch 58/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.7168 - acc: 0.5965\n",
      " [0.7329048918228879, 0.7956876456876457]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.7153 - acc: 0.5969\n",
      "Epoch 59/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.7179 - acc: 0.5910\n",
      "Epoch 60/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.6432 - acc: 0.6035\n",
      "Epoch 61/201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/80 [============================>.] - ETA: 0s - loss: 1.6714 - acc: 0.5993\n",
      " [0.7356553755285807, 0.7987179487179488]\n",
      "80/80 [==============================] - 40s 498ms/step - loss: 1.6680 - acc: 0.5996\n",
      "Epoch 62/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.6546 - acc: 0.5980\n",
      "Epoch 63/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.6866 - acc: 0.5906\n",
      "Epoch 64/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5901 - acc: 0.6203\n",
      " [0.7465982511389465, 0.7947552447552447]\n",
      "80/80 [==============================] - 38s 480ms/step - loss: 1.5904 - acc: 0.6203\n",
      "Epoch 65/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.6665 - acc: 0.5953\n",
      "Epoch 66/201\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 1.6424 - acc: 0.6105\n",
      "Epoch 67/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.6143 - acc: 0.6056\n",
      " [0.7416082740501712, 0.7987179487179488]\n",
      "80/80 [==============================] - 39s 484ms/step - loss: 1.6140 - acc: 0.6059\n",
      "Epoch 68/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 1.6335 - acc: 0.6020\n",
      "Epoch 69/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.6529 - acc: 0.5922\n",
      "Epoch 70/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.6205 - acc: 0.5953\n",
      " [0.7496162260385485, 0.7945221445221445]\n",
      "80/80 [==============================] - 39s 482ms/step - loss: 1.6273 - acc: 0.5938\n",
      "Epoch 71/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 1.5873 - acc: 0.6031\n",
      "Epoch 72/201\n",
      "80/80 [==============================] - 9s 117ms/step - loss: 1.5882 - acc: 0.6098\n",
      "Epoch 73/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.6266 - acc: 0.5934\n",
      " [0.7591571376322755, 0.796969696969697]\n",
      "80/80 [==============================] - 39s 487ms/step - loss: 1.6232 - acc: 0.5934\n",
      "Epoch 74/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.6254 - acc: 0.5836\n",
      "Epoch 75/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5682 - acc: 0.6008\n",
      "Epoch 76/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.6121 - acc: 0.5985\n",
      " [0.7537686384578115, 0.7995337995337995]\n",
      "80/80 [==============================] - 40s 497ms/step - loss: 1.6072 - acc: 0.6004\n",
      "Epoch 77/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.5253 - acc: 0.6258\n",
      "Epoch 78/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.6324 - acc: 0.5906\n",
      "Epoch 79/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5825 - acc: 0.6076\n",
      " [0.7548459994441582, 0.7967365967365967]\n",
      "80/80 [==============================] - 39s 482ms/step - loss: 1.5792 - acc: 0.6090\n",
      "Epoch 80/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.6062 - acc: 0.5996\n",
      "Epoch 81/201\n",
      "80/80 [==============================] - 10s 120ms/step - loss: 1.5585 - acc: 0.6098\n",
      "Epoch 82/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5631 - acc: 0.6092\n",
      " [0.7674777119286652, 0.7965034965034965]\n",
      "80/80 [==============================] - 39s 486ms/step - loss: 1.5585 - acc: 0.6109\n",
      "Epoch 83/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.5635 - acc: 0.6023\n",
      "Epoch 84/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 1.5469 - acc: 0.6168\n",
      "Epoch 85/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5124 - acc: 0.6206\n",
      " [0.7663904534837594, 0.7982517482517483]\n",
      "80/80 [==============================] - 39s 493ms/step - loss: 1.5089 - acc: 0.6215\n",
      "Epoch 86/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.5397 - acc: 0.6254\n",
      "Epoch 87/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.5631 - acc: 0.6207\n",
      "Epoch 88/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5352 - acc: 0.6183\n",
      " [0.7604348453996851, 0.799067599067599]\n",
      "80/80 [==============================] - 39s 482ms/step - loss: 1.5342 - acc: 0.6199\n",
      "Epoch 89/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5481 - acc: 0.6129\n",
      "Epoch 90/201\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.4935 - acc: 0.6281\n",
      "Epoch 91/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5573 - acc: 0.5977\n",
      " [0.7749748335411638, 0.7987179487179488]\n",
      "80/80 [==============================] - 39s 486ms/step - loss: 1.5500 - acc: 0.5988\n",
      "Epoch 92/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.5284 - acc: 0.6203\n",
      "Epoch 93/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.5549 - acc: 0.6145\n",
      "Epoch 94/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5214 - acc: 0.6076\n",
      " [0.7719503866463523, 0.8004662004662004]\n",
      "80/80 [==============================] - 40s 496ms/step - loss: 1.5233 - acc: 0.6063\n",
      "Epoch 95/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5248 - acc: 0.6227\n",
      "Epoch 96/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4876 - acc: 0.6301\n",
      "Epoch 97/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5008 - acc: 0.6159\n",
      " [0.7771729182629843, 0.7991841491841492]\n",
      "80/80 [==============================] - 38s 481ms/step - loss: 1.5022 - acc: 0.6160\n",
      "Epoch 98/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5091 - acc: 0.6191\n",
      "Epoch 99/201\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.5238 - acc: 0.6121\n",
      "Epoch 100/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4480 - acc: 0.6290\n",
      " [0.7911366969294661, 0.796037296037296]\n",
      "80/80 [==============================] - 39s 486ms/step - loss: 1.4443 - acc: 0.6309\n",
      "Epoch 101/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.5058 - acc: 0.6188\n",
      "Epoch 102/201\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 1.4918 - acc: 0.6234\n",
      "Epoch 103/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5024 - acc: 0.6238\n",
      " [0.7786415358460365, 0.7982517482517483]\n",
      "80/80 [==============================] - 39s 482ms/step - loss: 1.5028 - acc: 0.6234\n",
      "Epoch 104/201\n",
      "80/80 [==============================] - 10s 129ms/step - loss: 1.5021 - acc: 0.6180\n",
      "Epoch 105/201\n",
      "80/80 [==============================] - 10s 121ms/step - loss: 1.5023 - acc: 0.6086\n",
      "Epoch 106/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4536 - acc: 0.6377\n",
      " [0.7898177640241018, 0.7954545454545454]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.4559 - acc: 0.6371\n",
      "Epoch 107/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.5093 - acc: 0.6273\n",
      "Epoch 108/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.4766 - acc: 0.6250\n",
      "Epoch 109/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4140 - acc: 0.6428\n",
      " [0.7896034775783978, 0.8001165501165501]\n",
      "80/80 [==============================] - 39s 487ms/step - loss: 1.4133 - acc: 0.6434\n",
      "Epoch 110/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4775 - acc: 0.6195\n",
      "Epoch 111/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4870 - acc: 0.6145\n",
      "Epoch 112/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4888 - acc: 0.6266\n",
      " [0.7839133578312231, 0.7994172494172495]\n",
      "80/80 [==============================] - 39s 482ms/step - loss: 1.4929 - acc: 0.6246\n",
      "Epoch 113/201\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 1.4584 - acc: 0.6242\n",
      "Epoch 114/201\n",
      "80/80 [==============================] - 10s 119ms/step - loss: 1.4801 - acc: 0.6270\n",
      "Epoch 115/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4397 - acc: 0.6353\n",
      " [0.78195735167094, 0.8031468531468532]\n",
      "80/80 [==============================] - 39s 493ms/step - loss: 1.4368 - acc: 0.6375\n",
      "Epoch 116/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4801 - acc: 0.6234\n",
      "Epoch 117/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4412 - acc: 0.6324\n",
      "Epoch 118/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4403 - acc: 0.6297\n",
      " [0.7940872107702335, 0.8001165501165501]\n",
      "80/80 [==============================] - 39s 492ms/step - loss: 1.4381 - acc: 0.6309\n",
      "Epoch 119/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4211 - acc: 0.6398\n",
      "Epoch 120/201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4772 - acc: 0.6270\n",
      "Epoch 121/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4206 - acc: 0.6297\n",
      " [0.7988863774123183, 0.7982517482517483]\n",
      "80/80 [==============================] - 38s 480ms/step - loss: 1.4322 - acc: 0.6281\n",
      "Epoch 122/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.4655 - acc: 0.6203\n",
      "Epoch 123/201\n",
      "80/80 [==============================] - 9s 115ms/step - loss: 1.4175 - acc: 0.6402\n",
      "Epoch 124/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4502 - acc: 0.6282\n",
      " [0.8022352628364646, 0.7977855477855478]\n",
      "80/80 [==============================] - 39s 484ms/step - loss: 1.4504 - acc: 0.6281\n",
      "Epoch 125/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4234 - acc: 0.6480\n",
      "Epoch 126/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4849 - acc: 0.6121\n",
      "Epoch 127/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3845 - acc: 0.6444\n",
      " [0.7935049974228978, 0.8024475524475524]\n",
      "80/80 [==============================] - 39s 493ms/step - loss: 1.3877 - acc: 0.6445\n",
      "Epoch 128/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4298 - acc: 0.6383\n",
      "Epoch 129/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4303 - acc: 0.6355\n",
      "Epoch 130/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4359 - acc: 0.6294\n",
      " [0.804192636279401, 0.7994172494172495]\n",
      "80/80 [==============================] - 38s 479ms/step - loss: 1.4356 - acc: 0.6289\n",
      "Epoch 131/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4261 - acc: 0.6312\n",
      "Epoch 132/201\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.4525 - acc: 0.6336\n",
      "Epoch 133/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4508 - acc: 0.6329\n",
      " [0.8012551521759249, 0.798951048951049]\n",
      "80/80 [==============================] - 39s 483ms/step - loss: 1.4510 - acc: 0.6340\n",
      "Epoch 134/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3955 - acc: 0.6266\n",
      "Epoch 135/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3996 - acc: 0.6367\n",
      "Epoch 136/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3802 - acc: 0.6337\n",
      " [0.8102318882430525, 0.798018648018648]\n",
      "80/80 [==============================] - 39s 492ms/step - loss: 1.3814 - acc: 0.6324\n",
      "Epoch 137/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4225 - acc: 0.6355\n",
      "Epoch 138/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4112 - acc: 0.6391\n",
      "Epoch 139/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3866 - acc: 0.6294\n",
      " [0.8147103335270177, 0.7956876456876457]\n",
      "80/80 [==============================] - 38s 476ms/step - loss: 1.3905 - acc: 0.6301\n",
      "Epoch 140/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3910 - acc: 0.6465\n",
      "Epoch 141/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.4380 - acc: 0.6168\n",
      "Epoch 142/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3836 - acc: 0.6472\n",
      " [0.8036152408246899, 0.8005827505827506]\n",
      "80/80 [==============================] - 38s 480ms/step - loss: 1.3843 - acc: 0.6469\n",
      "Epoch 143/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3875 - acc: 0.6379\n",
      "Epoch 144/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4212 - acc: 0.6285\n",
      "Epoch 145/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3733 - acc: 0.6440\n",
      " [0.812604450177064, 0.801048951048951]\n",
      "80/80 [==============================] - 38s 479ms/step - loss: 1.3736 - acc: 0.6441\n",
      "Epoch 146/201\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.3631 - acc: 0.6453\n",
      "Epoch 147/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.3963 - acc: 0.6336\n",
      "Epoch 148/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3916 - acc: 0.6361\n",
      " [0.8230921459531342, 0.7982517482517483]\n",
      "80/80 [==============================] - 39s 487ms/step - loss: 1.3919 - acc: 0.6367\n",
      "Epoch 149/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4011 - acc: 0.6437\n",
      "Epoch 150/201\n",
      "80/80 [==============================] - 9s 115ms/step - loss: 1.3564 - acc: 0.6461\n",
      "Epoch 151/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3894 - acc: 0.6424\n",
      " [0.8196403395186777, 0.798018648018648]\n",
      "80/80 [==============================] - 39s 489ms/step - loss: 1.3832 - acc: 0.6445\n",
      "Epoch 152/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.4111 - acc: 0.6250\n",
      "Epoch 153/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3520 - acc: 0.6426\n",
      "Epoch 154/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3936 - acc: 0.6456\n",
      " [0.8125446665224054, 0.7995337995337995]\n",
      "80/80 [==============================] - 38s 477ms/step - loss: 1.3930 - acc: 0.6453\n",
      "Epoch 155/201\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.3710 - acc: 0.6473\n",
      "Epoch 156/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3480 - acc: 0.6426\n",
      "Epoch 157/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3485 - acc: 0.6551\n",
      " [0.8156657375720324, 0.798951048951049]\n",
      "80/80 [==============================] - 39s 482ms/step - loss: 1.3461 - acc: 0.6562\n",
      "Epoch 158/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.4051 - acc: 0.6488\n",
      "Epoch 159/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3359 - acc: 0.6547\n",
      "Epoch 160/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3683 - acc: 0.6523\n",
      " [0.8213957196389973, 0.8018648018648019]\n",
      "80/80 [==============================] - 39s 494ms/step - loss: 1.3627 - acc: 0.6531\n",
      "Epoch 161/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3508 - acc: 0.6492\n",
      "Epoch 162/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3483 - acc: 0.6430\n",
      "Epoch 163/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3520 - acc: 0.6594\n",
      " [0.825699260475994, 0.7982517482517483]\n",
      "80/80 [==============================] - 38s 479ms/step - loss: 1.3479 - acc: 0.6598\n",
      "Epoch 164/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3653 - acc: 0.6379\n",
      "Epoch 165/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 1.3661 - acc: 0.6480\n",
      "Epoch 166/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3340 - acc: 0.6436\n",
      " [0.8127918853237281, 0.801048951048951]\n",
      "80/80 [==============================] - 39s 486ms/step - loss: 1.3322 - acc: 0.6437\n",
      "Epoch 167/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3373 - acc: 0.6500\n",
      "Epoch 168/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3298 - acc: 0.6566\n",
      "Epoch 169/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3953 - acc: 0.6357\n",
      " [0.8287120488045207, 0.7991841491841492]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.3959 - acc: 0.6367\n",
      "Epoch 170/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.3166 - acc: 0.6582\n",
      "Epoch 171/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3357 - acc: 0.6645\n",
      "Epoch 172/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3472 - acc: 0.6503\n",
      " [0.8225865551506666, 0.8005827505827506]\n",
      "80/80 [==============================] - 39s 486ms/step - loss: 1.3446 - acc: 0.6520\n",
      "Epoch 173/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3194 - acc: 0.6527\n",
      "Epoch 174/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.3417 - acc: 0.6453\n",
      "Epoch 175/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3499 - acc: 0.6566\n",
      " [0.8303661473352637, 0.7997668997668997]\n",
      "80/80 [==============================] - 38s 481ms/step - loss: 1.3441 - acc: 0.6578\n",
      "Epoch 176/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3457 - acc: 0.6527\n",
      "Epoch 177/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3510 - acc: 0.6508\n",
      "Epoch 178/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3263 - acc: 0.6535\n",
      " [0.8303049500648487, 0.7991841491841492]\n",
      "80/80 [==============================] - 38s 476ms/step - loss: 1.3285 - acc: 0.6539\n",
      "Epoch 179/201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 10s 123ms/step - loss: 1.3276 - acc: 0.6457\n",
      "Epoch 180/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3169 - acc: 0.6477\n",
      "Epoch 181/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3476 - acc: 0.6456\n",
      " [0.8278498791053893, 0.8004662004662004]\n",
      "80/80 [==============================] - 38s 480ms/step - loss: 1.3500 - acc: 0.6453\n",
      "Epoch 182/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3688 - acc: 0.6414\n",
      "Epoch 183/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.3219 - acc: 0.6477\n",
      "Epoch 184/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3382 - acc: 0.6472\n",
      " [0.838619410536265, 0.798018648018648]\n",
      "80/80 [==============================] - 38s 481ms/step - loss: 1.3433 - acc: 0.6465\n",
      "Epoch 185/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3307 - acc: 0.6449\n",
      "Epoch 186/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3091 - acc: 0.6648\n",
      "Epoch 187/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2962 - acc: 0.6626\n",
      " [0.8312374604396617, 0.7995337995337995]\n",
      "80/80 [==============================] - 38s 479ms/step - loss: 1.2959 - acc: 0.6633\n",
      "Epoch 188/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.3366 - acc: 0.6578\n",
      "Epoch 189/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3416 - acc: 0.6523\n",
      "Epoch 190/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3258 - acc: 0.6503\n",
      " [0.8363911332433193, 0.8002331002331002]\n",
      "80/80 [==============================] - 39s 483ms/step - loss: 1.3280 - acc: 0.6508\n",
      "Epoch 191/201\n",
      "80/80 [==============================] - 9s 115ms/step - loss: 1.3287 - acc: 0.6508\n",
      "Epoch 192/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.2849 - acc: 0.6656\n",
      "Epoch 193/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2712 - acc: 0.6669\n",
      " [0.8418397006874514, 0.7974358974358975]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 1.2694 - acc: 0.6668\n",
      "Epoch 194/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3363 - acc: 0.6418\n",
      "Epoch 195/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3245 - acc: 0.6520\n",
      "Epoch 196/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3031 - acc: 0.6582\n",
      " [0.8371722416945655, 0.7996503496503496]\n",
      "80/80 [==============================] - 38s 479ms/step - loss: 1.3030 - acc: 0.6578\n",
      "Epoch 197/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.2614 - acc: 0.6730\n",
      "Epoch 198/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3433 - acc: 0.6535\n",
      "Epoch 199/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3625 - acc: 0.6412\n",
      " [0.8432053854109893, 0.8001165501165501]\n",
      "80/80 [==============================] - 39s 481ms/step - loss: 1.3615 - acc: 0.6418\n",
      "Epoch 200/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2881 - acc: 0.6578\n",
      "Epoch 201/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2903 - acc: 0.6496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcbc2374f28>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    Train, \n",
    "    steps_per_epoch=80, \n",
    "    epochs=201, \n",
    "    verbose=1, \n",
    "    workers=4,\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=20,\n",
    "    initial_epoch=0,\n",
    "    callbacks=[SaveModelandEval()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10080 images belonging to 120 classes.\n",
      "Found 1920 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "ImgGen = image.ImageDataGenerator(preprocessing_function=inception_v3.preprocess_input, \n",
    "                                  validation_split=1/6.)\n",
    "\n",
    "TrainF = ImgGen.flow_from_directory(join(prefix, imgfix, 'Train'), \n",
    "                                   target_size=(224, 224), \n",
    "                                   class_mode='sparse', \n",
    "                                   batch_size=32, \n",
    "                                   shuffle=True, \n",
    "                                   seed=None, \n",
    "                                   subset='training', \n",
    "                                   interpolation='nearest')\n",
    "ValidF = ImgGen.flow_from_directory(join(prefix, imgfix, 'Train'), \n",
    "                                   target_size=(224, 224), \n",
    "                                   class_mode='sparse', \n",
    "                                   batch_size=32, \n",
    "                                   shuffle=True, \n",
    "                                   seed=None, \n",
    "                                   subset='validation', \n",
    "                                   interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('inceptionv3_dfc120_114_8031')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[279:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(lr=0.0001, momentum=0.9), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "class SaveModelandEval(callbacks.Callback):\n",
    "    prev_res = list()\n",
    "    def __init__(self):\n",
    "        self.prev_res = [0., 0.]\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch == 0: return\n",
    "        if epoch % 3 == 0:\n",
    "            res = model.evaluate_generator(Test, verbose=0, steps=None, workers=4, use_multiprocessing=True, max_queue_size=12)\n",
    "            print('\\n', res)\n",
    "            if res[1] > self.prev_res[1]:\n",
    "                models.save_model(model, 'inceptionv3_dfc120_ft_%s_%s'%(epoch, int(res[1]*10000)) )\n",
    "                self.prev_res = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 3 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, None, 3 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, None, 3 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 3 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 3 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 6 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 6 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 6 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, None, None, 6 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 8 5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 8 240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 8 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 1 138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 1 576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 1 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 1 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 6 192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 4 9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 9 55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 4 144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 9 288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 4 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 9 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, None, None, 1 0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 6 76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 9 82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 3 6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 6 192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 6 192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 9 288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 3 96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 6 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 6 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 9 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 3 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, None, None, 2 0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 6 192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, None, 6 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 4 12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 9 55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 4 144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, None, 9 288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 4 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, None, 9 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, None, None, 2 0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 6 76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 9 82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 6 16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 6 192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 6 192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, None, 9 288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, None, 6 192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 6 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 6 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, None, 9 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, None, 6 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, None, None, 2 0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, None, 6 192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 6 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 4 13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 9 55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, None, 4 144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, None, 9 288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, None, 4 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 9 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, None, None, 2 0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 6 76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 9 82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 6 18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, None, 6 192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, None, 6 192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, None, 9 288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, None, 6 192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, None, 6 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 6 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 9 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 6 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, None, None, 2 0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 6 18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, None, 6 192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, None, 6 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, None, None, 9 55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, None, 9 288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, None, 9 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 3 995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, None, None, 9 82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, None, 3 1152        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, None, 9 288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 3 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, None, 9 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 2 0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, None, None, 7 0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 1 384         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, None, None, 1 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, None, None, 1 114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 1 384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, None, 1 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, None, None, 1 114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 1 384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 1 384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, None, None, 1 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, None, 1 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, None, None, 1 114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, None, None, 1 114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 1 384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 1 384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, None, None, 1 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, None, None, 1 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, None, None, 7 0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, None, None, 1 147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, None, None, 1 172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, None, None, 1 172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 1 576         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 1 576         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, None, 1 576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 1 576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, None, 1 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, None, None, 1 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, None, None, 1 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, None, None, 1 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, None, None, 7 0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, None, 1 480         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, None, None, 1 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, None, None, 1 179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, None, 1 480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, None, None, 1 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 1 179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, None, 1 480         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, None, 1 480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, None, None, 1 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, None, None, 1 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, None, None, 1 179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 1 179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, None, 1 480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, None, 1 480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, None, None, 1 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, None, None, 1 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, None, None, 7 0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, None, None, 1 147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, None, None, 1 215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 1 215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 1 576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, None, 1 576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, None, 1 576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, None, 1 576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, None, None, 1 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, None, None, 1 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, None, None, 1 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, None, None, 1 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, None, None, 7 0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, None, None, 1 480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, None, None, 1 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, None, None, 1 179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, None, None, 1 480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, None, None, 1 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, None, None, 1 179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, None, None, 1 480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, None, None, 1 480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, None, None, 1 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, None, None, 1 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 1 179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, None, None, 1 179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, None, None, 1 480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, None, None, 1 480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, None, None, 1 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, None, 1 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, None, None, 7 0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 1 147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 1 215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, None, None, 1 215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, None, None, 1 576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, None, None, 1 576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, None, 1 576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, None, 1 576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, None, None, 1 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, None, None, 1 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, None, 1 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, None, None, 1 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, None, None, 7 0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, None, None, 1 576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, None, None, 1 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, None, None, 1 258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, None, None, 1 576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, None, None, 1 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, None, None, 1 258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, None, 1 576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, None, None, 1 576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, None, None, 1 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, None, None, 1 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, None, None, 1 258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, None, None, 1 258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, None, None, 1 576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, None, None, 1 576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, None, None, 1 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, None, None, 1 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, None, None, 7 0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, None, None, 1 258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, None, None, 1 258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, None, 1 576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, None, None, 1 576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, None, None, 1 576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, None, None, 1 576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, None, None, 1 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, None, None, 1 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, None, None, 1 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, None, None, 1 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, None, None, 7 0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, None, None, 1 576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, None, None, 1 0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, None, None, 1 258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, None, None, 1 576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, None, None, 1 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, None, None, 1 258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, None, None, 1 576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, None, None, 1 576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, None, None, 1 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, None, None, 1 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, None, None, 3 552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, None, None, 1 331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, None, None, 3 960         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, None, None, 1 576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, None, None, 3 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, None, None, 1 0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 7 0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, None, None, 1 0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, None, None, 4 573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, None, None, 4 1344        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, None, None, 4 0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, None, None, 3 491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, None, None, 3 1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, None, None, 3 1152        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, None, None, 3 1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, None, None, 3 0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, None, None, 3 0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, None, None, 3 442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, None, None, 3 442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, None, None, 3 442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, None, None, 3 442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, None, None, 1 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, None, None, 3 409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, None, None, 3 1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, None, None, 3 1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, None, None, 3 1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, None, None, 3 1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, None, None, 1 245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, None, None, 3 960         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, None, None, 3 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, None, None, 3 0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, None, None, 3 0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, None, None, 3 0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, None, None, 1 576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, None, None, 3 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 7 0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, None, None, 1 0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, None, None, 2 0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, None, None, 4 917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, None, None, 4 1344        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, None, None, 4 0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, None, None, 3 786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, None, None, 3 1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, None, None, 3 1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, None, None, 3 1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, None, None, 3 0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, None, None, 3 0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, None, None, 3 442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, None, None, 3 442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, None, None, 3 442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, None, None, 3 442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, None, None, 2 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, None, None, 3 655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, None, None, 3 1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, None, None, 3 1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, None, None, 3 1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, None, None, 3 1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, None, None, 1 393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, None, None, 3 960         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, None, None, 3 0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, None, None, 3 0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, None, None, 3 0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, None, None, 3 0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, None, None, 1 576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, None, None, 3 0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 7 0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, None, None, 1 0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, None, None, 2 0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_22 (Gl (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 2048)         0           global_average_pooling2d_22[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 120)          245880      dropout_19[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 22,048,664\n",
      "Trainable params: 6,319,416\n",
      "Non-trainable params: 15,729,248\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/41\n",
      "80/80 [==============================] - 17s 212ms/step - loss: 1.3406 - acc: 0.6629\n",
      "Epoch 2/41\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 1.3778 - acc: 0.6492\n",
      "Epoch 3/41\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 1.3596 - acc: 0.6508\n",
      "Epoch 4/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3367 - acc: 0.6543\n",
      " [0.7473589497563257, 0.8005827505827506]\n",
      "80/80 [==============================] - 98s 1s/step - loss: 1.3304 - acc: 0.6562\n",
      "Epoch 5/41\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.2873 - acc: 0.6680\n",
      "Epoch 6/41\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.2797 - acc: 0.6746\n",
      "Epoch 7/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2775 - acc: 0.6744\n",
      " [0.7494967815984633, 0.7983682983682984]\n",
      "80/80 [==============================] - 39s 485ms/step - loss: 1.2761 - acc: 0.6750\n",
      "Epoch 8/41\n",
      "80/80 [==============================] - 11s 133ms/step - loss: 1.2481 - acc: 0.6676\n",
      "Epoch 9/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.2127 - acc: 0.6891\n",
      "Epoch 10/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2772 - acc: 0.6646\n",
      " [0.7561367714985946, 0.7962703962703963]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.2766 - acc: 0.6648\n",
      "Epoch 11/41\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.2690 - acc: 0.6727\n",
      "Epoch 12/41\n",
      "80/80 [==============================] - 11s 134ms/step - loss: 1.2122 - acc: 0.6859\n",
      "Epoch 13/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1964 - acc: 0.6990\n",
      " [0.7557928654611488, 0.7963869463869464]\n",
      "80/80 [==============================] - 39s 489ms/step - loss: 1.1916 - acc: 0.7008\n",
      "Epoch 14/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.2156 - acc: 0.6758\n",
      "Epoch 15/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.2155 - acc: 0.6809\n",
      "Epoch 16/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2180 - acc: 0.6764\n",
      " [0.7564726258687635, 0.7952214452214452]\n",
      "80/80 [==============================] - 40s 504ms/step - loss: 1.2183 - acc: 0.6777\n",
      "Epoch 17/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.1512 - acc: 0.6996\n",
      "Epoch 18/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.2025 - acc: 0.6891\n",
      "Epoch 19/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2036 - acc: 0.6824\n",
      " [0.7562030515172, 0.7972027972027972]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.2032 - acc: 0.6820\n",
      "Epoch 20/41\n",
      "80/80 [==============================] - 11s 134ms/step - loss: 1.1786 - acc: 0.6895\n",
      "Epoch 21/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.1297 - acc: 0.7016\n",
      "Epoch 22/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1275 - acc: 0.6950\n",
      " [0.761240740457956, 0.7945221445221445]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.1257 - acc: 0.6949\n",
      "Epoch 23/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.1659 - acc: 0.6855\n",
      "Epoch 24/41\n",
      "80/80 [==============================] - 11s 134ms/step - loss: 1.1088 - acc: 0.7086\n",
      "Epoch 25/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1148 - acc: 0.7029\n",
      " [0.7663551524761376, 0.7926573426573427]\n",
      "80/80 [==============================] - 39s 489ms/step - loss: 1.1105 - acc: 0.7055\n",
      "Epoch 26/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.0943 - acc: 0.7137\n",
      "Epoch 27/41\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.0839 - acc: 0.7148\n",
      "Epoch 28/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1100 - acc: 0.7160\n",
      " [0.7607340812731646, 0.7948717948717948]\n",
      "80/80 [==============================] - 40s 502ms/step - loss: 1.1148 - acc: 0.7156\n",
      "Epoch 29/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.0764 - acc: 0.7133\n",
      "Epoch 30/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.0952 - acc: 0.7113\n",
      "Epoch 31/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.0717 - acc: 0.7227\n",
      " [0.7617013178660281, 0.794055944055944]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 1.0711 - acc: 0.7234\n",
      "Epoch 32/41\n",
      "80/80 [==============================] - 11s 136ms/step - loss: 1.0797 - acc: 0.7215\n",
      "Epoch 33/41\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.0643 - acc: 0.7254\n",
      "Epoch 34/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.0825 - acc: 0.7093\n",
      " [0.7590411020426494, 0.7962703962703963]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.0810 - acc: 0.7094\n",
      "Epoch 35/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.0321 - acc: 0.7258\n",
      "Epoch 36/41\n",
      "80/80 [==============================] - 11s 135ms/step - loss: 1.0286 - acc: 0.7145\n",
      "Epoch 37/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.0024 - acc: 0.7389\n",
      " [0.7592452706076044, 0.7951048951048951]\n",
      "80/80 [==============================] - 39s 489ms/step - loss: 1.0032 - acc: 0.7387\n",
      "Epoch 38/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.0632 - acc: 0.7160\n",
      "Epoch 39/41\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.0428 - acc: 0.7203\n",
      "Epoch 40/41\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.0202 - acc: 0.7318\n",
      " [0.7652654233802502, 0.7928904428904429]\n",
      "80/80 [==============================] - 40s 499ms/step - loss: 1.0203 - acc: 0.7328\n",
      "Epoch 41/41\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 0.9734 - acc: 0.7434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcb74541f98>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    TrainF, \n",
    "    steps_per_epoch=80, \n",
    "    epochs=41, \n",
    "    verbose=1, \n",
    "    workers=4,\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=20,\n",
    "    initial_epoch=0,\n",
    "    callbacks=[SaveModelandEval()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inceptionv3_modified = models.load_model('fc_d240120_20')\n",
    "inceptionv3_modified.compile(\n",
    "        optimizer='SGD', \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_modified.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(inceptionv3_modified.evaluate_generator(Train, verbose=1, workers=4, use_multiprocessing=True, max_queue_size=12) )\n",
    "print(inceptionv3_modified.evaluate_generator(Valid, verbose=1, workers=4, use_multiprocessing=True, max_queue_size=12) )\n",
    "print(inceptionv3_modified.evaluate_generator(Test,  verbose=1, workers=4, use_multiprocessing=True, max_queue_size=12) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_modified.fit_generator(\n",
    "    Train, \n",
    "    steps_per_epoch=312, \n",
    "    epochs=41, \n",
    "    verbose=1, \n",
    "    workers=4,\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=20,\n",
    "#     validation_data=Valid, \n",
    "#     validation_steps=60, \n",
    "    initial_epoch=21,\n",
    "    callbacks=[SaveModelandEval()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = inceptionv3_naive.input\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(inceptionv3_naive.output)\n",
    "print(x.shape)\n",
    "\n",
    "f_1 = layers.Dense(64)(x)\n",
    "f_2 = layers.Dense(64, activation='sigmoid')(x)\n",
    "feature = layers.multiply([f_1, f_2])\n",
    "print(feature.shape)\n",
    "\n",
    "predict = layers.Dense(120, activation='sigmoid', name='softmax1')(feature)\n",
    "auxiliary = layers.Dense(120, activation='sigmoid', name='auxiliary')(x)\n",
    "\n",
    "print(predict.shape)\n",
    "print(auxiliary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_target = layers.Input(shape=(1,))\n",
    "centers = layers.Embedding(120, 64)(input_target)\n",
    "l2_loss = layers.Lambda(lambda x: K.sum(K.square(x[0]-x[1][:,0]), 1, keepdims=True), name='l2')([feature, centers])\n",
    "\n",
    "print(input_target.shape)\n",
    "print(centers.shape)\n",
    "print(l2_loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Concatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs \n",
    "input_image = inceptionv3_naive.input\n",
    "\n",
    "# Intermediate Layers\n",
    "x1 = inceptionv3_naive.output\n",
    "x1 = layers.GlobalAveragePooling2D()(x1)\n",
    "# x2 = inceptionv3_naive.get_layer('mixed7').output\n",
    "x2 = inceptionv3_naive.get_layer('mixed9').output\n",
    "x2 = layers.GlobalAveragePooling2D()(x2)\n",
    "x = layers.Concatenate(axis=-1)([x1,x2])\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(120, activation='softmax')(x)\n",
    "\n",
    "# Compile\n",
    "model = models.Model(inputs=input_image, outputs=x)\n",
    "for layer in inceptionv3_naive.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer='SGD', \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "class SaveModelandEval(callbacks.Callback):\n",
    "    prev_res = list()\n",
    "    def __init__(self):\n",
    "        self.prev_res = [0., 0.]\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch == 0: return\n",
    "        if epoch % 3 == 0:\n",
    "            res = model.evaluate_generator(Test, verbose=0, steps=None, workers=4, use_multiprocessing=True, max_queue_size=12)\n",
    "            print('\\n', res)\n",
    "            if res[1] > self.prev_res[1]:\n",
    "                models.save_model(model, 'inceptionv3_d2fc120_%s_%s'%(epoch, int(res[1]*10000)) )\n",
    "                self.prev_res = res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/201\n",
      "80/80 [==============================] - 16s 203ms/step - loss: 4.7924 - acc: 0.0172\n",
      "Epoch 2/201\n",
      "80/80 [==============================] - 9s 110ms/step - loss: 4.5486 - acc: 0.0496\n",
      "Epoch 3/201\n",
      "80/80 [==============================] - 9s 116ms/step - loss: 4.3404 - acc: 0.1258\n",
      "Epoch 4/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 4.1264 - acc: 0.1748\n",
      " [2.690066069267291, 0.5432400932400933]\n",
      "80/80 [==============================] - 46s 580ms/step - loss: 4.1247 - acc: 0.1758\n",
      "Epoch 5/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 3.8846 - acc: 0.2668\n",
      "Epoch 6/201\n",
      "80/80 [==============================] - 9s 115ms/step - loss: 3.6992 - acc: 0.3031\n",
      "Epoch 7/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 3.4904 - acc: 0.3592\n",
      " [1.6523953325126, 0.6815850815850816]\n",
      "80/80 [==============================] - 40s 497ms/step - loss: 3.4870 - acc: 0.3602\n",
      "Epoch 8/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 3.3593 - acc: 0.3906\n",
      "Epoch 9/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 3.2397 - acc: 0.3871\n",
      "Epoch 10/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 3.0777 - acc: 0.4296\n",
      " [1.1606727974398152, 0.7181818181818181]\n",
      "80/80 [==============================] - 40s 500ms/step - loss: 3.0759 - acc: 0.4309\n",
      "Epoch 11/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 2.9169 - acc: 0.4547\n",
      "Epoch 12/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 2.8486 - acc: 0.4680\n",
      "Epoch 13/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.7457 - acc: 0.4703\n",
      " [0.9173939433409062, 0.747086247086247]\n",
      "80/80 [==============================] - 39s 492ms/step - loss: 2.7428 - acc: 0.4703\n",
      "Epoch 14/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 2.6846 - acc: 0.4730\n",
      "Epoch 15/201\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 2.6016 - acc: 0.4855\n",
      "Epoch 16/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.5128 - acc: 0.4960\n",
      " [0.8044876655585421, 0.7628205128205128]\n",
      "80/80 [==============================] - 40s 499ms/step - loss: 2.5108 - acc: 0.4969\n",
      "Epoch 17/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 2.4395 - acc: 0.5109\n",
      "Epoch 18/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 2.3982 - acc: 0.5082\n",
      "Epoch 19/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.3556 - acc: 0.5119\n",
      " [0.7638033520308756, 0.7684149184149184]\n",
      "80/80 [==============================] - 40s 504ms/step - loss: 2.3555 - acc: 0.5113\n",
      "Epoch 20/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 2.2797 - acc: 0.5402\n",
      "Epoch 21/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 2.2710 - acc: 0.5301\n",
      "Epoch 22/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.2230 - acc: 0.5174\n",
      " [0.7186511371432346, 0.7771561771561771]\n",
      "80/80 [==============================] - 40s 494ms/step - loss: 2.2245 - acc: 0.5172\n",
      "Epoch 23/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 2.1702 - acc: 0.5449\n",
      "Epoch 24/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 2.1776 - acc: 0.5187\n",
      "Epoch 25/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.1045 - acc: 0.5431\n",
      " [0.7118650827434192, 0.777972027972028]\n",
      "80/80 [==============================] - 40s 498ms/step - loss: 2.1055 - acc: 0.5426\n",
      "Epoch 26/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 2.0915 - acc: 0.5516\n",
      "Epoch 27/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 2.0441 - acc: 0.5422\n",
      "Epoch 28/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2.0317 - acc: 0.5542\n",
      " [0.7133592289439913, 0.7787878787878788]\n",
      "80/80 [==============================] - 40s 497ms/step - loss: 2.0339 - acc: 0.5547\n",
      "Epoch 29/201\n",
      "80/80 [==============================] - 11s 132ms/step - loss: 1.9953 - acc: 0.5402\n",
      "Epoch 30/201\n",
      "80/80 [==============================] - 10s 120ms/step - loss: 2.0087 - acc: 0.5418\n",
      "Epoch 31/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.9305 - acc: 0.5759\n",
      " [0.6975395283966797, 0.7878787878787878]\n",
      "80/80 [==============================] - 40s 502ms/step - loss: 1.9319 - acc: 0.5758\n",
      "Epoch 32/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.9692 - acc: 0.5520\n",
      "Epoch 33/201\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.9291 - acc: 0.5445\n",
      "Epoch 34/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.9377 - acc: 0.5491\n",
      " [0.7091330044943023, 0.7854312354312354]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.9327 - acc: 0.5512\n",
      "Epoch 35/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.9093 - acc: 0.5613\n",
      "Epoch 36/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.8542 - acc: 0.5855\n",
      "Epoch 37/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.9055 - acc: 0.5522\n",
      " [0.7075496321825184, 0.7878787878787878]\n",
      "80/80 [==============================] - 39s 485ms/step - loss: 1.9046 - acc: 0.5531\n",
      "Epoch 38/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 1.8625 - acc: 0.5598\n",
      "Epoch 39/201\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 1.8716 - acc: 0.5723\n",
      "Epoch 40/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.8300 - acc: 0.5712\n",
      " [0.7088234209357859, 0.7897435897435897]\n",
      "80/80 [==============================] - 40s 500ms/step - loss: 1.8309 - acc: 0.5707\n",
      "Epoch 41/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.7869 - acc: 0.5836\n",
      "Epoch 42/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.7910 - acc: 0.5785\n",
      "Epoch 43/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.8124 - acc: 0.5744\n",
      " [0.7091647945253813, 0.7927738927738928]\n",
      "80/80 [==============================] - 40s 506ms/step - loss: 1.8103 - acc: 0.5754\n",
      "Epoch 44/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.7477 - acc: 0.5879\n",
      "Epoch 45/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.8034 - acc: 0.5766\n",
      "Epoch 46/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.7658 - acc: 0.5862\n",
      " [0.7167833146723156, 0.7926573426573427]\n",
      "80/80 [==============================] - 39s 484ms/step - loss: 1.7632 - acc: 0.5871\n",
      "Epoch 47/201\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.7483 - acc: 0.5754\n",
      "Epoch 48/201\n",
      "80/80 [==============================] - 9s 118ms/step - loss: 1.7747 - acc: 0.5727\n",
      "Epoch 49/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.7239 - acc: 0.5934\n",
      " [0.7276597366749828, 0.7906759906759907]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.7267 - acc: 0.5918\n",
      "Epoch 50/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.7114 - acc: 0.5973\n",
      "Epoch 51/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.7229 - acc: 0.5801\n",
      "Epoch 52/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.7152 - acc: 0.5878\n",
      " [0.7176581686663025, 0.7935897435897435]\n",
      "80/80 [==============================] - 40s 503ms/step - loss: 1.7110 - acc: 0.5883\n",
      "Epoch 53/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.6837 - acc: 0.5883\n",
      "Epoch 54/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.7221 - acc: 0.5793\n",
      "Epoch 55/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.7161 - acc: 0.5850\n",
      " [0.7252468890169598, 0.793939393939394]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 1.7110 - acc: 0.5859\n",
      "Epoch 56/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.6679 - acc: 0.6090\n",
      "Epoch 57/201\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 1.6697 - acc: 0.5941\n",
      "Epoch 58/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.6661 - acc: 0.5953\n",
      " [0.7356253177213856, 0.7920745920745921]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.6642 - acc: 0.5953\n",
      "Epoch 59/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.6634 - acc: 0.6023\n",
      "Epoch 60/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.6417 - acc: 0.5918\n",
      "Epoch 61/201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/80 [============================>.] - ETA: 0s - loss: 1.6724 - acc: 0.5870\n",
      " [0.7392309925202112, 0.7917249417249417]\n",
      "80/80 [==============================] - 40s 494ms/step - loss: 1.6709 - acc: 0.5863\n",
      "Epoch 62/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.6291 - acc: 0.5906\n",
      "Epoch 63/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.6331 - acc: 0.6043\n",
      "Epoch 64/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.6215 - acc: 0.6036\n",
      " [0.7334173941463482, 0.7970862470862471]\n",
      "80/80 [==============================] - 39s 493ms/step - loss: 1.6239 - acc: 0.6035\n",
      "Epoch 65/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.6142 - acc: 0.6039\n",
      "Epoch 66/201\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.6407 - acc: 0.5949\n",
      "Epoch 67/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5952 - acc: 0.6088\n",
      " [0.74178794795008, 0.7956876456876457]\n",
      "80/80 [==============================] - 39s 489ms/step - loss: 1.5929 - acc: 0.6098\n",
      "Epoch 68/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.6174 - acc: 0.6027\n",
      "Epoch 69/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5751 - acc: 0.6121\n",
      "Epoch 70/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5827 - acc: 0.5965\n",
      " [0.7409022253371377, 0.7976689976689977]\n",
      "80/80 [==============================] - 40s 496ms/step - loss: 1.5805 - acc: 0.5965\n",
      "Epoch 71/201\n",
      "80/80 [==============================] - 10s 130ms/step - loss: 1.6041 - acc: 0.6016\n",
      "Epoch 72/201\n",
      "80/80 [==============================] - 10s 120ms/step - loss: 1.5565 - acc: 0.6223\n",
      "Epoch 73/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5585 - acc: 0.6036\n",
      " [0.7515054761841294, 0.7974358974358975]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.5595 - acc: 0.6039\n",
      "Epoch 74/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.6418 - acc: 0.5957\n",
      "Epoch 75/201\n",
      "80/80 [==============================] - 9s 116ms/step - loss: 1.5794 - acc: 0.6160\n",
      "Epoch 76/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5304 - acc: 0.6226\n",
      " [0.7533095881013044, 0.7981351981351982]\n",
      "80/80 [==============================] - 40s 500ms/step - loss: 1.5231 - acc: 0.6250\n",
      "Epoch 77/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.6075 - acc: 0.5969\n",
      "Epoch 78/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.5705 - acc: 0.6164\n",
      "Epoch 79/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5515 - acc: 0.6179\n",
      " [0.7600412393358026, 0.794988344988345]\n",
      "80/80 [==============================] - 39s 489ms/step - loss: 1.5532 - acc: 0.6172\n",
      "Epoch 80/201\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.5604 - acc: 0.6102\n",
      "Epoch 81/201\n",
      "80/80 [==============================] - 9s 119ms/step - loss: 1.5484 - acc: 0.6102\n",
      "Epoch 82/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5301 - acc: 0.6112\n",
      " [0.7578351625417862, 0.7979020979020979]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.5284 - acc: 0.6117\n",
      "Epoch 83/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5240 - acc: 0.6129\n",
      "Epoch 84/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.5476 - acc: 0.6141\n",
      "Epoch 85/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5102 - acc: 0.6242\n",
      " [0.7601756301000326, 0.7965034965034965]\n",
      "80/80 [==============================] - 39s 493ms/step - loss: 1.5118 - acc: 0.6242\n",
      "Epoch 86/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5378 - acc: 0.6125\n",
      "Epoch 87/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5211 - acc: 0.6152\n",
      "Epoch 88/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5511 - acc: 0.6127\n",
      " [0.7667887318057675, 0.7973193473193473]\n",
      "80/80 [==============================] - 38s 479ms/step - loss: 1.5483 - acc: 0.6141\n",
      "Epoch 89/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5305 - acc: 0.6215\n",
      "Epoch 90/201\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.5123 - acc: 0.6168\n",
      "Epoch 91/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5070 - acc: 0.6131\n",
      " [0.7703079309126434, 0.7973193473193473]\n",
      "80/80 [==============================] - 39s 484ms/step - loss: 1.5091 - acc: 0.6129\n",
      "Epoch 92/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5099 - acc: 0.6180\n",
      "Epoch 93/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5586 - acc: 0.6070\n",
      "Epoch 94/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.5067 - acc: 0.6214\n",
      " [0.7624217970690389, 0.8027972027972028]\n",
      "80/80 [==============================] - 40s 502ms/step - loss: 1.5015 - acc: 0.6227\n",
      "Epoch 95/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5096 - acc: 0.6184\n",
      "Epoch 96/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.5012 - acc: 0.6184\n",
      "Epoch 97/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4990 - acc: 0.6187\n",
      " [0.7810528352768104, 0.7970862470862471]\n",
      "80/80 [==============================] - 39s 482ms/step - loss: 1.4953 - acc: 0.6188\n",
      "Epoch 98/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4815 - acc: 0.6254\n",
      "Epoch 99/201\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 1.5007 - acc: 0.6137\n",
      "Epoch 100/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4624 - acc: 0.6297\n",
      " [0.7710697475287633, 0.8]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.4639 - acc: 0.6277\n",
      "Epoch 101/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4780 - acc: 0.6289\n",
      "Epoch 102/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.5096 - acc: 0.6219\n",
      "Epoch 103/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4748 - acc: 0.6274\n",
      " [0.775210091318843, 0.7996503496503496]\n",
      "80/80 [==============================] - 38s 480ms/step - loss: 1.4754 - acc: 0.6281\n",
      "Epoch 104/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 1.4877 - acc: 0.6164\n",
      "Epoch 105/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.4392 - acc: 0.6293\n",
      "Epoch 106/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4805 - acc: 0.6226\n",
      " [0.7791605859183542, 0.7982517482517483]\n",
      "80/80 [==============================] - 39s 485ms/step - loss: 1.4776 - acc: 0.6230\n",
      "Epoch 107/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4513 - acc: 0.6266\n",
      "Epoch 108/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 1.4613 - acc: 0.6262\n",
      "Epoch 109/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4732 - acc: 0.6258\n",
      " [0.7883837848509424, 0.7952214452214452]\n",
      "80/80 [==============================] - 39s 489ms/step - loss: 1.4737 - acc: 0.6266\n",
      "Epoch 110/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4705 - acc: 0.6230\n",
      "Epoch 111/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4892 - acc: 0.6246\n",
      "Epoch 112/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4573 - acc: 0.6325\n",
      " [0.781104345964396, 0.7991841491841492]\n",
      "80/80 [==============================] - 39s 485ms/step - loss: 1.4601 - acc: 0.6312\n",
      "Epoch 113/201\n",
      "80/80 [==============================] - 10s 130ms/step - loss: 1.4014 - acc: 0.6457\n",
      "Epoch 114/201\n",
      "80/80 [==============================] - 9s 117ms/step - loss: 1.4273 - acc: 0.6379\n",
      "Epoch 115/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4137 - acc: 0.6301\n",
      " [0.7793654316961629, 0.7988344988344989]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.4145 - acc: 0.6289\n",
      "Epoch 116/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4416 - acc: 0.6352\n",
      "Epoch 117/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.4480 - acc: 0.6289\n",
      "Epoch 118/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4297 - acc: 0.6286\n",
      " [0.7813746294267045, 0.8022144522144522]\n",
      "80/80 [==============================] - 40s 495ms/step - loss: 1.4280 - acc: 0.6301\n",
      "Epoch 119/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4008 - acc: 0.6410\n",
      "Epoch 120/201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4521 - acc: 0.6332\n",
      "Epoch 121/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4609 - acc: 0.6206\n",
      " [0.7865523778525179, 0.7983682983682984]\n",
      "80/80 [==============================] - 39s 486ms/step - loss: 1.4588 - acc: 0.6215\n",
      "Epoch 122/201\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.4255 - acc: 0.6332\n",
      "Epoch 123/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.4301 - acc: 0.6426\n",
      "Epoch 124/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4390 - acc: 0.6341\n",
      " [0.7911933501392253, 0.7982517482517483]\n",
      "80/80 [==============================] - 40s 494ms/step - loss: 1.4400 - acc: 0.6340\n",
      "Epoch 125/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3935 - acc: 0.6324\n",
      "Epoch 126/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3852 - acc: 0.6387\n",
      "Epoch 127/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4531 - acc: 0.6270\n",
      " [0.7986434563471861, 0.7963869463869464]\n",
      "80/80 [==============================] - 40s 498ms/step - loss: 1.4530 - acc: 0.6273\n",
      "Epoch 128/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.4131 - acc: 0.6332\n",
      "Epoch 129/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3719 - acc: 0.6340\n",
      "Epoch 130/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4201 - acc: 0.6353\n",
      " [0.7917391113110834, 0.8001165501165501]\n",
      "80/80 [==============================] - 39s 485ms/step - loss: 1.4193 - acc: 0.6352\n",
      "Epoch 131/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.4217 - acc: 0.6344\n",
      "Epoch 132/201\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.4163 - acc: 0.6328\n",
      "Epoch 133/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3827 - acc: 0.6321\n",
      " [0.8002718912655905, 0.796037296037296]\n",
      "80/80 [==============================] - 39s 493ms/step - loss: 1.3786 - acc: 0.6344\n",
      "Epoch 134/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3986 - acc: 0.6406\n",
      "Epoch 135/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3792 - acc: 0.6336\n",
      "Epoch 136/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4128 - acc: 0.6183\n",
      " [0.8043698843050241, 0.7988344988344989]\n",
      "80/80 [==============================] - 39s 492ms/step - loss: 1.4078 - acc: 0.6191\n",
      "Epoch 137/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.4143 - acc: 0.6324\n",
      "Epoch 138/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3942 - acc: 0.6395\n",
      "Epoch 139/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.4145 - acc: 0.6377\n",
      " [0.8055192697840174, 0.7979020979020979]\n",
      "80/80 [==============================] - 39s 484ms/step - loss: 1.4168 - acc: 0.6367\n",
      "Epoch 140/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3984 - acc: 0.6391\n",
      "Epoch 141/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 1.4199 - acc: 0.6297\n",
      "Epoch 142/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3832 - acc: 0.6353\n",
      " [0.8081233464929964, 0.7962703962703963]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.3854 - acc: 0.6363\n",
      "Epoch 143/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3977 - acc: 0.6527\n",
      "Epoch 144/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3629 - acc: 0.6527\n",
      "Epoch 145/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3560 - acc: 0.6551\n",
      " [0.8002684201808674, 0.7970862470862471]\n",
      "80/80 [==============================] - 39s 484ms/step - loss: 1.3544 - acc: 0.6551\n",
      "Epoch 146/201\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.3612 - acc: 0.6367\n",
      "Epoch 147/201\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 1.4004 - acc: 0.6434\n",
      "Epoch 148/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3536 - acc: 0.6475\n",
      " [0.8171876497551979, 0.7952214452214452]\n",
      "80/80 [==============================] - 39s 487ms/step - loss: 1.3511 - acc: 0.6484\n",
      "Epoch 149/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3655 - acc: 0.6441\n",
      "Epoch 150/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.4128 - acc: 0.6320\n",
      "Epoch 151/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3761 - acc: 0.6404\n",
      " [0.8085394652524872, 0.8003496503496503]\n",
      "80/80 [==============================] - 40s 495ms/step - loss: 1.3740 - acc: 0.6406\n",
      "Epoch 152/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3607 - acc: 0.6465\n",
      "Epoch 153/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3571 - acc: 0.6422\n",
      "Epoch 154/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3391 - acc: 0.6420\n",
      " [0.8144922719040105, 0.7982517482517483]\n",
      "80/80 [==============================] - 39s 487ms/step - loss: 1.3382 - acc: 0.6426\n",
      "Epoch 155/201\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.3584 - acc: 0.6496\n",
      "Epoch 156/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.3769 - acc: 0.6387\n",
      "Epoch 157/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3430 - acc: 0.6578\n",
      " [0.8222661192301347, 0.7951048951048951]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 1.3412 - acc: 0.6570\n",
      "Epoch 158/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.4109 - acc: 0.6340\n",
      "Epoch 159/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3291 - acc: 0.6527\n",
      "Epoch 160/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3347 - acc: 0.6479\n",
      " [0.8116213675182951, 0.8018648018648019]\n",
      "80/80 [==============================] - 40s 498ms/step - loss: 1.3358 - acc: 0.6480\n",
      "Epoch 161/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3069 - acc: 0.6547\n",
      "Epoch 162/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3663 - acc: 0.6441\n",
      "Epoch 163/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3904 - acc: 0.6373\n",
      " [0.8167621343872408, 0.7997668997668997]\n",
      "80/80 [==============================] - 39s 485ms/step - loss: 1.3971 - acc: 0.6359\n",
      "Epoch 164/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.3285 - acc: 0.6582\n",
      "Epoch 165/201\n",
      "80/80 [==============================] - 10s 130ms/step - loss: 1.3292 - acc: 0.6430\n",
      "Epoch 166/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3482 - acc: 0.6452\n",
      " [0.814724712825211, 0.8009324009324009]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.3484 - acc: 0.6445\n",
      "Epoch 167/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3827 - acc: 0.6340\n",
      "Epoch 168/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3003 - acc: 0.6645\n",
      "Epoch 169/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3271 - acc: 0.6511\n",
      " [0.8256375256026316, 0.7981351981351982]\n",
      "80/80 [==============================] - 39s 492ms/step - loss: 1.3298 - acc: 0.6508\n",
      "Epoch 170/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3551 - acc: 0.6566\n",
      "Epoch 171/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3044 - acc: 0.6660\n",
      "Epoch 172/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3381 - acc: 0.6384\n",
      " [0.8195711879533865, 0.7994172494172495]\n",
      "80/80 [==============================] - 39s 483ms/step - loss: 1.3390 - acc: 0.6379\n",
      "Epoch 173/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.3718 - acc: 0.6387\n",
      "Epoch 174/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 1.3211 - acc: 0.6512\n",
      "Epoch 175/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3343 - acc: 0.6519\n",
      " [0.8317700846493162, 0.7988344988344989]\n",
      "80/80 [==============================] - 39s 489ms/step - loss: 1.3360 - acc: 0.6516\n",
      "Epoch 176/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3178 - acc: 0.6602\n",
      "Epoch 177/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3408 - acc: 0.6496\n",
      "Epoch 178/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2941 - acc: 0.6555\n",
      " [0.8257443996255511, 0.798018648018648]\n",
      "80/80 [==============================] - 39s 484ms/step - loss: 1.2962 - acc: 0.6543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/201\n",
      "80/80 [==============================] - 10s 130ms/step - loss: 1.3292 - acc: 0.6473\n",
      "Epoch 180/201\n",
      "80/80 [==============================] - 10s 119ms/step - loss: 1.3509 - acc: 0.6539\n",
      "Epoch 181/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3071 - acc: 0.6511\n",
      " [0.8269413478832236, 0.7997668997668997]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.3030 - acc: 0.6523\n",
      "Epoch 182/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3479 - acc: 0.6469\n",
      "Epoch 183/201\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.3126 - acc: 0.6512\n",
      "Epoch 184/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2444 - acc: 0.6752\n",
      " [0.8241954635696714, 0.8004662004662004]\n",
      "80/80 [==============================] - 39s 489ms/step - loss: 1.2489 - acc: 0.6746\n",
      "Epoch 185/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3242 - acc: 0.6578\n",
      "Epoch 186/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3193 - acc: 0.6488\n",
      "Epoch 187/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3187 - acc: 0.6491\n",
      " [0.8245885425242181, 0.8022144522144522]\n",
      "80/80 [==============================] - 39s 485ms/step - loss: 1.3194 - acc: 0.6496\n",
      "Epoch 188/201\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 1.3271 - acc: 0.6465\n",
      "Epoch 189/201\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 1.2972 - acc: 0.6512\n",
      "Epoch 190/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3297 - acc: 0.6444\n",
      " [0.8341819665852159, 0.7991841491841492]\n",
      "80/80 [==============================] - 39s 494ms/step - loss: 1.3275 - acc: 0.6449\n",
      "Epoch 191/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2769 - acc: 0.6660\n",
      "Epoch 192/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.3335 - acc: 0.6426\n",
      "Epoch 193/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2698 - acc: 0.6614\n",
      " [0.825614761193309, 0.7998834498834498]\n",
      "80/80 [==============================] - 40s 501ms/step - loss: 1.2673 - acc: 0.6617\n",
      "Epoch 194/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2876 - acc: 0.6648\n",
      "Epoch 195/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.3131 - acc: 0.6574\n",
      "Epoch 196/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3122 - acc: 0.6610\n",
      " [0.8293920828189991, 0.7984848484848485]\n",
      "80/80 [==============================] - 39s 485ms/step - loss: 1.3091 - acc: 0.6613\n",
      "Epoch 197/201\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.2570 - acc: 0.6703\n",
      "Epoch 198/201\n",
      "80/80 [==============================] - 10s 120ms/step - loss: 1.3289 - acc: 0.6492\n",
      "Epoch 199/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2828 - acc: 0.6594\n",
      " [0.8326861169147357, 0.801981351981352]\n",
      "80/80 [==============================] - 39s 493ms/step - loss: 1.2844 - acc: 0.6590\n",
      "Epoch 200/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.2678 - acc: 0.6652\n",
      "Epoch 201/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2560 - acc: 0.6691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcb6f847d68>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    Train, \n",
    "    steps_per_epoch=80, \n",
    "    epochs=201, \n",
    "    verbose=1, \n",
    "    workers=4,\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=20,\n",
    "    initial_epoch=0,\n",
    "    callbacks=[SaveModelandEval()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with the GLU activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2048)\n",
      "(?, 240)\n",
      "(?, 120)\n"
     ]
    }
   ],
   "source": [
    "## Input\n",
    "input_image = inceptionv3_naive.input\n",
    "\n",
    "## Intermediate Layers\n",
    "x = layers.GlobalAveragePooling2D()(inceptionv3_naive.output)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "f_1 = layers.Dense(240)(x)\n",
    "f_2 = layers.Dense(240, activation='sigmoid')(x)\n",
    "feature = layers.multiply([f_1, f_2])\n",
    "\n",
    "predict = layers.Dense(120, activation='sigmoid', name='softmax1')(feature)\n",
    "\n",
    "print(x.shape)\n",
    "print(feature.shape)\n",
    "print(predict.shape)\n",
    "\n",
    "## Compile \n",
    "model = models.Model(inputs=[input_image], outputs=[predict])\n",
    "for layer in inceptionv3_naive.layers:\n",
    "    layer.trainable = False\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss=['sparse_categorical_crossentropy'], \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "## Callbacks\n",
    "class SaveModelandEval(callbacks.Callback):\n",
    "    prev_res = []\n",
    "    def __init__(self):\n",
    "        self.prev_res = [0., 0.]\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch == 0: return\n",
    "        if epoch % 3 == 0:\n",
    "            res = model.evaluate_generator(Test, verbose=0, steps=None, workers=4, use_multiprocessing=True, max_queue_size=12)\n",
    "            print('\\n', res)\n",
    "            if res[1] > self.prev_res[1]:\n",
    "                models.save_model(model, 'inceptionv3_dglu240fc120_%s_%s'%(epoch, int(res[1]*10000)) )\n",
    "                self.prev_res = res\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/201\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 1.2371 - acc: 0.6559\n",
      "Epoch 103/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2399 - acc: 0.6428\n",
      " [1.106701986140801, 0.7663170163170163]\n",
      "80/80 [==============================] - 39s 486ms/step - loss: 1.2447 - acc: 0.6414\n",
      "Epoch 104/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.2617 - acc: 0.6555\n",
      "Epoch 105/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.3364 - acc: 0.6172\n",
      "Epoch 106/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2025 - acc: 0.6574\n",
      " [1.221794537927791, 0.7648018648018649]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.2050 - acc: 0.6562\n",
      "Epoch 107/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.2231 - acc: 0.6586\n",
      "Epoch 108/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.2223 - acc: 0.6457\n",
      "Epoch 109/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3463 - acc: 0.6408\n",
      " [1.2163290164998248, 0.759090909090909]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 1.3431 - acc: 0.6426\n",
      "Epoch 110/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2982 - acc: 0.6320\n",
      "Epoch 111/201\n",
      "80/80 [==============================] - 10s 129ms/step - loss: 1.2116 - acc: 0.6727\n",
      "Epoch 112/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2942 - acc: 0.6408\n",
      " [1.1522890321325556, 0.7664335664335664]\n",
      "80/80 [==============================] - 40s 494ms/step - loss: 1.2993 - acc: 0.6398\n",
      "Epoch 113/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2311 - acc: 0.6609\n",
      "Epoch 114/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2340 - acc: 0.6543\n",
      "Epoch 115/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3121 - acc: 0.6428\n",
      " [1.1863342700790833, 0.7653846153846153]\n",
      "80/80 [==============================] - 38s 481ms/step - loss: 1.3100 - acc: 0.6434\n",
      "Epoch 116/201\n",
      "80/80 [==============================] - 10s 129ms/step - loss: 1.1996 - acc: 0.6543\n",
      "Epoch 117/201\n",
      "80/80 [==============================] - 10s 121ms/step - loss: 1.2681 - acc: 0.6457\n",
      "Epoch 118/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2411 - acc: 0.6559\n",
      " [1.1914932853125961, 0.7671328671328671]\n",
      "80/80 [==============================] - 40s 502ms/step - loss: 1.2468 - acc: 0.6539\n",
      "Epoch 119/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2516 - acc: 0.6617\n",
      "Epoch 120/201\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 1.3077 - acc: 0.6379\n",
      "Epoch 121/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1853 - acc: 0.6622\n",
      " [1.2153517276603898, 0.7651515151515151]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.1883 - acc: 0.6613\n",
      "Epoch 122/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2695 - acc: 0.6555\n",
      "Epoch 123/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2772 - acc: 0.6527\n",
      "Epoch 124/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3136 - acc: 0.6250\n",
      " [1.165973537227297, 0.7757575757575758]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 1.3089 - acc: 0.6262\n",
      "Epoch 125/201\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 1.1796 - acc: 0.6617\n",
      "Epoch 126/201\n",
      "80/80 [==============================] - 10s 121ms/step - loss: 1.2384 - acc: 0.6609\n",
      "Epoch 127/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2561 - acc: 0.6507\n",
      " [1.2124083406496307, 0.7642191142191143]\n",
      "80/80 [==============================] - 39s 493ms/step - loss: 1.2566 - acc: 0.6500\n",
      "Epoch 128/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2942 - acc: 0.6355\n",
      "Epoch 129/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.2915 - acc: 0.6277\n",
      "Epoch 130/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2189 - acc: 0.6547\n",
      " [1.2616307101463453, 0.7624708624708625]\n",
      "80/80 [==============================] - 40s 499ms/step - loss: 1.2162 - acc: 0.6555\n",
      "Epoch 131/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2271 - acc: 0.6609\n",
      "Epoch 132/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2133 - acc: 0.6543\n",
      "Epoch 133/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.3223 - acc: 0.6377\n",
      " [1.2380960764432487, 0.7671328671328671]\n",
      "80/80 [==============================] - 39s 486ms/step - loss: 1.3215 - acc: 0.6379\n",
      "Epoch 134/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 1.2775 - acc: 0.6492\n",
      "Epoch 135/201\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 1.2295 - acc: 0.6398\n",
      "Epoch 136/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2666 - acc: 0.6519\n",
      " [1.184241568437009, 0.7648018648018649]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 1.2685 - acc: 0.6504\n",
      "Epoch 137/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.2017 - acc: 0.6680\n",
      "Epoch 138/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2239 - acc: 0.6621\n",
      "Epoch 139/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2056 - acc: 0.6622\n",
      " [1.229150737526686, 0.7664335664335664]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 1.2036 - acc: 0.6625\n",
      "Epoch 140/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1494 - acc: 0.6680\n",
      "Epoch 141/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1969 - acc: 0.6637\n",
      "Epoch 142/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2673 - acc: 0.6424\n",
      " [1.1068595402494719, 0.7703962703962703]\n",
      "80/80 [==============================] - 39s 485ms/step - loss: 1.2665 - acc: 0.6437\n",
      "Epoch 143/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.2476 - acc: 0.6512\n",
      "Epoch 144/201\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 1.1599 - acc: 0.6691\n",
      "Epoch 145/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1985 - acc: 0.6685\n",
      " [1.266739641993087, 0.7680652680652681]\n",
      "80/80 [==============================] - 39s 487ms/step - loss: 1.2004 - acc: 0.6672\n",
      "Epoch 146/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2208 - acc: 0.6602\n",
      "Epoch 147/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.2047 - acc: 0.6699\n",
      "Epoch 148/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2325 - acc: 0.6646\n",
      " [1.1852266286566435, 0.7682983682983683]\n",
      "80/80 [==============================] - 40s 501ms/step - loss: 1.2303 - acc: 0.6648\n",
      "Epoch 149/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.1503 - acc: 0.6699\n",
      "Epoch 150/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2202 - acc: 0.6555\n",
      "Epoch 151/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2089 - acc: 0.6689\n",
      " [1.1909286312495648, 0.7717948717948718]\n",
      "80/80 [==============================] - 38s 481ms/step - loss: 1.2191 - acc: 0.6664\n",
      "Epoch 152/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2497 - acc: 0.6371\n",
      "Epoch 153/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 1.2051 - acc: 0.6516\n",
      "Epoch 154/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1395 - acc: 0.6792\n",
      " [1.3196029485155794, 0.7575757575757576]\n",
      "80/80 [==============================] - 39s 487ms/step - loss: 1.1359 - acc: 0.6805\n",
      "Epoch 155/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1549 - acc: 0.6789\n",
      "Epoch 156/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.2768 - acc: 0.6430\n",
      "Epoch 157/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2630 - acc: 0.6381\n",
      " [1.2369387367655142, 0.764918414918415]\n",
      "80/80 [==============================] - 39s 487ms/step - loss: 1.2621 - acc: 0.6383\n",
      "Epoch 158/201\n",
      "80/80 [==============================] - 10s 128ms/step - loss: 1.2025 - acc: 0.6602\n",
      "Epoch 159/201\n",
      "80/80 [==============================] - 10s 121ms/step - loss: 1.1292 - acc: 0.6695\n",
      "Epoch 160/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1559 - acc: 0.6681\n",
      " [1.206596950692083, 0.7685314685314686]\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 1.1553 - acc: 0.6680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2168 - acc: 0.6527\n",
      "Epoch 162/201\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 1.2138 - acc: 0.6484\n",
      "Epoch 163/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1577 - acc: 0.6792\n",
      " [1.2131502959635603, 0.7615384615384615]\n",
      "80/80 [==============================] - 39s 487ms/step - loss: 1.1588 - acc: 0.6793\n",
      "Epoch 164/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1966 - acc: 0.6516\n",
      "Epoch 165/201\n",
      "80/80 [==============================] - 9s 112ms/step - loss: 1.2092 - acc: 0.6652\n",
      "Epoch 166/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2365 - acc: 0.6527\n",
      " [1.2548714658587945, 0.7637529137529138]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.2334 - acc: 0.6535\n",
      "Epoch 167/201\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.1807 - acc: 0.6641\n",
      "Epoch 168/201\n",
      "80/80 [==============================] - 10s 121ms/step - loss: 1.1251 - acc: 0.6715\n",
      "Epoch 169/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1666 - acc: 0.6701\n",
      " [1.209474569517326, 0.775990675990676]\n",
      "80/80 [==============================] - 40s 496ms/step - loss: 1.1713 - acc: 0.6691\n",
      "Epoch 170/201\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 1.2085 - acc: 0.6535\n",
      "Epoch 171/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2263 - acc: 0.6539\n",
      "Epoch 172/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1792 - acc: 0.6610\n",
      " [1.245248613052339, 0.7747086247086247]\n",
      "80/80 [==============================] - 40s 496ms/step - loss: 1.1785 - acc: 0.6617\n",
      "Epoch 173/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1209 - acc: 0.6832\n",
      "Epoch 174/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1756 - acc: 0.6715\n",
      "Epoch 175/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.2462 - acc: 0.6460\n",
      " [1.2682251015464823, 0.752913752913753]\n",
      "80/80 [==============================] - 39s 484ms/step - loss: 1.2483 - acc: 0.6453\n",
      "Epoch 176/201\n",
      "80/80 [==============================] - 9s 115ms/step - loss: 1.1900 - acc: 0.6691\n",
      "Epoch 177/201\n",
      "80/80 [==============================] - 10s 126ms/step - loss: 1.1621 - acc: 0.6660\n",
      "Epoch 178/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1614 - acc: 0.6760\n",
      " [1.2421270782679024, 0.777039627039627]\n",
      "80/80 [==============================] - 40s 494ms/step - loss: 1.1566 - acc: 0.6777\n",
      "Epoch 179/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2115 - acc: 0.6578\n",
      "Epoch 180/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2508 - acc: 0.6488\n",
      "Epoch 181/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1450 - acc: 0.6800\n",
      " [1.2111258284756137, 0.7712121212121212]\n",
      "80/80 [==============================] - 40s 498ms/step - loss: 1.1435 - acc: 0.6801\n",
      "Epoch 182/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1010 - acc: 0.6801\n",
      "Epoch 183/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1671 - acc: 0.6680\n",
      "Epoch 184/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1914 - acc: 0.6570\n",
      " [1.3109904392574652, 0.7594405594405594]\n",
      "80/80 [==============================] - 39s 487ms/step - loss: 1.1920 - acc: 0.6574\n",
      "Epoch 185/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1646 - acc: 0.6633\n",
      "Epoch 186/201\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 1.1263 - acc: 0.6805\n",
      "Epoch 187/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1027 - acc: 0.6903\n",
      " [1.2413169295374746, 0.772027972027972]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 1.0996 - acc: 0.6891\n",
      "Epoch 188/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1874 - acc: 0.6668\n",
      "Epoch 189/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2067 - acc: 0.6512\n",
      "Epoch 190/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1593 - acc: 0.6713\n",
      " [1.2760512226000036, 0.768997668997669]\n",
      "80/80 [==============================] - 39s 488ms/step - loss: 1.1631 - acc: 0.6703\n",
      "Epoch 191/201\n",
      "80/80 [==============================] - 10s 130ms/step - loss: 1.1179 - acc: 0.6789\n",
      "Epoch 192/201\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 1.2000 - acc: 0.6598\n",
      "Epoch 193/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1732 - acc: 0.6642\n",
      " [1.2552174759633186, 0.7764568764568764]\n",
      "80/80 [==============================] - 39s 489ms/step - loss: 1.1739 - acc: 0.6637\n",
      "Epoch 194/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1934 - acc: 0.6551\n",
      "Epoch 195/201\n",
      "80/80 [==============================] - 10s 122ms/step - loss: 1.1585 - acc: 0.6707\n",
      "Epoch 196/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1050 - acc: 0.6772\n",
      " [1.3092846123724127, 0.7573426573426574]\n",
      "80/80 [==============================] - 39s 491ms/step - loss: 1.1118 - acc: 0.6766\n",
      "Epoch 197/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1392 - acc: 0.6789\n",
      "Epoch 198/201\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.1765 - acc: 0.6641\n",
      "Epoch 199/201\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1.1834 - acc: 0.6693\n",
      " [1.2605523103044491, 0.7643356643356644]\n",
      "80/80 [==============================] - 39s 489ms/step - loss: 1.1842 - acc: 0.6699\n",
      "Epoch 200/201\n",
      "80/80 [==============================] - 10s 129ms/step - loss: 1.1343 - acc: 0.6719\n",
      "Epoch 201/201\n",
      "80/80 [==============================] - 10s 119ms/step - loss: 1.0653 - acc: 0.6953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcbb0d14198>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    Train, \n",
    "    steps_per_epoch=80, \n",
    "    epochs=201, \n",
    "    verbose=1, \n",
    "    workers=4,\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=20,\n",
    "    initial_epoch=101,\n",
    "    callbacks=[SaveModelandEval()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate_generator(Train, verbose=1, workers=4, use_multiprocessing=True, max_queue_size=12) )\n",
    "print(model.evaluate_generator(Valid, verbose=1, workers=4, use_multiprocessing=True, max_queue_size=12) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLU + Center Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input\n",
    "input_image = inceptionv3_naive.input\n",
    "input_target = layers.Input(shape=(1,))\n",
    "\n",
    "## Intermediate Layers\n",
    "x = layers.GlobalAveragePooling2D()(inceptionv3_naive.output)\n",
    "\n",
    "f_1 = layers.Dense(64)(x)\n",
    "f_2 = layers.Dense(64, activation='sigmoid')(x)\n",
    "feature = layers.multiply([f_1, f_2])\n",
    "\n",
    "predict = layers.Dense(120, activation='sigmoid', name='softmax')(feature)\n",
    "auxiliary = layers.Dense(120, activation='sigmoid', name='auxiliary')(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(feature.shape)\n",
    "print(predict.shape)\n",
    "print(auxiliary.shape)\n",
    "\n",
    "centers = layers.Embedding(120, 64)(input_target)\n",
    "l2_loss = layers.Lambda(lambda x: K.sum(K.square(x[0]-x[1][:,0]), 1, keepdims=True), name='l2')([feature, centers])\n",
    "\n",
    "print(centers.shape)\n",
    "print(l2_loss.shape)\n",
    "\n",
    "## Compile \n",
    "model = models.Model(inputs=[input_image, input_target], outputs=[predict, l2_loss, auxiliary])\n",
    "for layer in inceptionv3_naive.layers:\n",
    "    layer.trainable = False\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss=['sparse_categorical_crossentropy', lambda y_true, y_pred: y_pred, 'sparse_categorical_crossentropy'], \n",
    "    loss_weights=[1., 0.25, 0.25], \n",
    "    metrics={'softmax':'accuracy','auxiliary':'accuracy'}\n",
    ")\n",
    "\n",
    "## Callbacks\n",
    "class SaveModelandEval(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch == 0: return\n",
    "        if epoch % 5 == 0:\n",
    "            models.save_model(model, 'inceptionv3_glu64c_120_'+str(epoch) )\n",
    "            print(model.evaluate_generator(TestData, steps=269, verbose=1, workers=4, use_multiprocessing=True, max_queue_size=12) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('inceptionv3_glu64c_120_20',\n",
    "                          custom_objects={'<lambda>': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "for layer in inceptionv3_naive.layers:\n",
    "    layer.trainable = False\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(lr=1e-4, momentum=0.9), \n",
    "    loss=['sparse_categorical_crossentropy', lambda y_true, y_pred: y_pred, 'sparse_categorical_crossentropy'], \n",
    "    loss_weights=[1., 0.25, 0.25], \n",
    "    metrics={'softmax':'accuracy','auxiliary':'accuracy'}\n",
    ")\n",
    "model.evaluate_generator(TestData, steps=269, verbose=1, workers=4, use_multiprocessing=True, max_queue_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    TrainData, \n",
    "    steps_per_epoch=312, \n",
    "    epochs=41, \n",
    "    verbose=1, \n",
    "    workers=4,\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=20,\n",
    "    initial_epoch=21,\n",
    "    callbacks=[SaveModelandEval()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.save_model(inceptionv3_modified, 'sample_model_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tunning Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_modified = models.load_model('fc12040')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed0 40\n",
      "mixed1 63\n",
      "mixed2 86\n",
      "mixed3 100\n",
      "mixed4 132\n",
      "mixed5 164\n",
      "mixed6 196\n",
      "mixed7 228\n",
      "mixed8 248\n",
      "mixed9_0 276\n",
      "mixed9 279\n",
      "mixed9_1 307\n",
      "mixed10 310\n",
      "\n",
      "\n",
      "\n",
      "Begin Configuring...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the layers infomation\n",
    "for i in range(len(inceptionv3_naive.layers)):\n",
    "    layer = inceptionv3_naive.layers[i]\n",
    "    layerType = str(layer.name)\n",
    "    if layerType[:5] == 'mixed':\n",
    "        print(layerType, i)\n",
    "\n",
    "print('\\n\\n\\nBegin Configuring...\\n\\n\\n')\n",
    "        \n",
    "# # Set the paramters of the last InceptionBlock to be trainable\n",
    "# for layer in inceptionv3_modified.layers[279:]:\n",
    "#     print(str(layer.name) )\n",
    "#     layer.trainable = True\n",
    "    \n",
    "# inceptionv3_modified.compile(\n",
    "#     optimizer=keras.optimizers.SGD(lr=0.0001, momentum=0.9), \n",
    "#     loss='categorical_crossentropy', \n",
    "#     metrics=['accuracy'])\n",
    "                             \n",
    "# inceptionv3_modified.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Each mixed layer split the InceptionBlock within the models. \n",
    "2. Start by fine tuning the last IncptionBlock. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModelandEval(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch == 0: return\n",
    "        if epoch % 20 == 0:\n",
    "            models.save_model(inceptionv3_modified, 'fc120_ft_'+str(epoch) )\n",
    "            print(inceptionv3_modified.evaluate_generator(Test,  verbose=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_modified = models.load_model('fc120_ft_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_modified.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_modified.fit_generator(\n",
    "    Train, \n",
    "    steps_per_epoch=312, \n",
    "    epochs=101, \n",
    "    verbose=1, \n",
    "    workers=4,\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=12,\n",
    "    validation_data=Valid, \n",
    "    validation_steps=60, \n",
    "    initial_epoch=61,\n",
    "    callbacks=[SaveModelandEval()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inceptionv3_modified.evaluate_generator(Train, verbose=1) )\n",
    "print(inceptionv3_modified.evaluate_generator(Valid, verbose=1) )\n",
    "print(inceptionv3_modified.evaluate_generator(Test,  verbose=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_prediction(Pred, Top=1):\n",
    "    res = []\n",
    "    for pred in Pred:\n",
    "        scores = [(pred[i], i) for i in range(120)]\n",
    "        scores.sort(reverse=1)\n",
    "        res.append(scores[:Top])\n",
    "    return res\n",
    "\n",
    "def calculate_accuracy(Pred, Lbl, Top):\n",
    "    idx, cnt = 0, 0\n",
    "    for res in decode_prediction(Pred, Top):\n",
    "        lbls = [res[i][1] for i in range(Top)]\n",
    "        if Lbl[idx] in lbls:\n",
    "            cnt += 1\n",
    "        idx += 1\n",
    "    return cnt / len(Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_modified = models.load_model('fc120_ft_40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = inceptionv3_modified.predict_generator(Test, verbose=1, workers=4, use_multiprocessing=True, max_queue_size=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 epochs\n",
    "print(calculate_accuracy(P, Test.classes, Top=1) )\n",
    "print(calculate_accuracy(P, Test.classes, Top=2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40 epochs\n",
    "print(calculate_accuracy(P, Test.classes, Top=1) )\n",
    "print(calculate_accuracy(P, Test.classes, Top=2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune for 20 epochs\n",
    "print(calculate_accuracy(P, Test.classes, Top=1) )\n",
    "print(calculate_accuracy(P, Test.classes, Top=2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune for 40 epochs\n",
    "print(calculate_accuracy(P, Test.classes, Top=1) )\n",
    "print(calculate_accuracy(P, Test.classes, Top=2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xception = keras.applications.Xception(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xception.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
