{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.6-tf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths, Datasets and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from os.path import join\n",
    "\n",
    "prefix = '../../Stanford_Dog_Breed'\n",
    "imgfix = 'Images'\n",
    "anofix = 'Annotation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set:\t 12000\n",
      "Size of the testing set: \t 8580\n"
     ]
    }
   ],
   "source": [
    "train = loadmat(join(prefix, 'train_list.mat'))\n",
    "test = loadmat(join(prefix, 'test_list.mat'))\n",
    "\n",
    "# L_annotation = L['annotation_list'];\n",
    "# L_file = L['file_list'];\n",
    "# L = L['labels'];\n",
    "\n",
    "ftrain = train['file_list']\n",
    "ftest = test['file_list']\n",
    "\n",
    "ltrain = train['labels']\n",
    "ltest = test['labels']\n",
    "\n",
    "ntrain = ltrain.shape[0]\n",
    "ntest = ltest.shape[0]\n",
    "\n",
    "print('Size of the training set:\\t', ntrain)\n",
    "print('Size of the testing set: \\t', ntest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catagory 2 Index Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chihuahua 1\n",
      "Japanese_spaniel 2\n",
      "Maltese_dog 3\n",
      "Pekinese 4\n",
      "Shih 5\n",
      "Blenheim_spaniel 6\n",
      "papillon 7\n",
      "toy_terrier 8\n",
      "Rhodesian_ridgeback 9\n",
      "Afghan_hound 10\n",
      "basset 11\n",
      "beagle 12\n",
      "bloodhound 13\n",
      "bluetick 14\n",
      "black 15\n",
      "Walker_hound 16\n",
      "English_foxhound 17\n",
      "redbone 18\n",
      "borzoi 19\n",
      "Irish_wolfhound 20\n",
      "Italian_greyhound 21\n",
      "whippet 22\n",
      "Ibizan_hound 23\n",
      "Norwegian_elkhound 24\n",
      "otterhound 25\n",
      "Saluki 26\n",
      "Scottish_deerhound 27\n",
      "Weimaraner 28\n",
      "Staffordshire_bullterrier 29\n",
      "American_Staffordshire_terrier 30\n",
      "Bedlington_terrier 31\n",
      "Border_terrier 32\n",
      "Kerry_blue_terrier 33\n",
      "Irish_terrier 34\n",
      "Norfolk_terrier 35\n",
      "Norwich_terrier 36\n",
      "Yorkshire_terrier 37\n",
      "wire 38\n",
      "Lakeland_terrier 39\n",
      "Sealyham_terrier 40\n",
      "Airedale 41\n",
      "cairn 42\n",
      "Australian_terrier 43\n",
      "Dandie_Dinmont 44\n",
      "Boston_bull 45\n",
      "miniature_schnauzer 46\n",
      "giant_schnauzer 47\n",
      "standard_schnauzer 48\n",
      "Scotch_terrier 49\n",
      "Tibetan_terrier 50\n",
      "silky_terrier 51\n",
      "soft 52\n",
      "West_Highland_white_terrier 53\n",
      "Lhasa 54\n",
      "flat 55\n",
      "curly 56\n",
      "golden_retriever 57\n",
      "Labrador_retriever 58\n",
      "Chesapeake_Bay_retriever 59\n",
      "German_short 60\n",
      "vizsla 61\n",
      "English_setter 62\n",
      "Irish_setter 63\n",
      "Gordon_setter 64\n",
      "Brittany_spaniel 65\n",
      "clumber 66\n",
      "English_springer 67\n",
      "Welsh_springer_spaniel 68\n",
      "cocker_spaniel 69\n",
      "Sussex_spaniel 70\n",
      "Irish_water_spaniel 71\n",
      "kuvasz 72\n",
      "schipperke 73\n",
      "groenendael 74\n",
      "malinois 75\n",
      "briard 76\n",
      "kelpie 77\n",
      "komondor 78\n",
      "Old_English_sheepdog 79\n",
      "Shetland_sheepdog 80\n",
      "collie 81\n",
      "Border_collie 82\n",
      "Bouvier_des_Flandres 83\n",
      "Rottweiler 84\n",
      "German_shepherd 85\n",
      "Doberman 86\n",
      "miniature_pinscher 87\n",
      "Greater_Swiss_Mountain_dog 88\n",
      "Bernese_mountain_dog 89\n",
      "Appenzeller 90\n",
      "EntleBucher 91\n",
      "boxer 92\n",
      "bull_mastiff 93\n",
      "Tibetan_mastiff 94\n",
      "French_bulldog 95\n",
      "Great_Dane 96\n",
      "Saint_Bernard 97\n",
      "Eskimo_dog 98\n",
      "malamute 99\n",
      "Siberian_husky 100\n",
      "affenpinscher 101\n",
      "basenji 102\n",
      "pug 103\n",
      "Leonberg 104\n",
      "Newfoundland 105\n",
      "Great_Pyrenees 106\n",
      "Samoyed 107\n",
      "Pomeranian 108\n",
      "chow 109\n",
      "keeshond 110\n",
      "Brabancon_griffon 111\n",
      "Pembroke 112\n",
      "Cardigan 113\n",
      "toy_poodle 114\n",
      "miniature_poodle 115\n",
      "standard_poodle 116\n",
      "Mexican_hairless 117\n",
      "dingo 118\n",
      "dhole 119\n",
      "African_hunting_dog 120\n"
     ]
    }
   ],
   "source": [
    "DogBreed = {}\n",
    "for i in range(1, ntrain):\n",
    "    l = ltrain[i][0]\n",
    "    c = ftrain[i][0][0].split('/')[0]\n",
    "    c = c.split('-')[1]\n",
    "    if c not in DogBreed:\n",
    "        DogBreed[c] = l\n",
    "        print(c, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_path(idx):\n",
    "    try:    path = join(prefix, imgfix, ftest[idx][0][0])\n",
    "    except: path = join(prefix, imgfix, ftest[-1][0][0])      \n",
    "    return  path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_labl(idx):\n",
    "    try:    labl = ltest[idx][0]\n",
    "    except: labl = ltest[-1][0]\n",
    "    return  labl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage exmaples for image classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. ResNet50\n",
    "    2. DenseNet\n",
    "    3. VGG19\n",
    "    4. InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calssify ImageNet classes with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground: French_bulldog\n",
      "Predicted: [('n02108915', 'French_bulldog', 0.93131804), ('n02096585', 'Boston_bull', 0.06433643), ('n02108089', 'boxer', 0.0027737769)]\n",
      "Found the class with French_bulldog and probability 0.93131804\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(range(ntest))\n",
    "path = gen_path(idx)\n",
    "labl = gen_labl(idx)\n",
    "\n",
    "print('Ground:', path.split('/')[4].split('-')[1])\n",
    "\n",
    "img = image.load_img(path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "preds = decode_predictions(preds, top=3)[0];\n",
    "\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "print('Predicted:', preds)\n",
    "\n",
    "for pred in preds:\n",
    "    c = pred[1];\n",
    "    if c not in DogBreed: continue\n",
    "    lb = DogBreed[c]\n",
    "    if lb == labl: \n",
    "        print('Found the class with', c, 'and probability', pred[2])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for idx in range(ntest):\n",
    "    path = gen_path(idx)\n",
    "    labl = gen_labl(idx)\n",
    "    try:\n",
    "        img = image.load_img(path, target_size=(224, 224))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        X.append(preprocess_input(x))\n",
    "    except:\n",
    "        print(idx)\n",
    "    \n",
    "XTest = np.vstack(X)\n",
    "YTest = model.predict(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8580, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(YTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6541\n",
      "0.7623543123543124\n"
     ]
    }
   ],
   "source": [
    "true = 0\n",
    "top = 1\n",
    "\n",
    "for idx in range(ntest):\n",
    "    pred = decode_predictions(YTest[idx:idx+1,:], top=top)[0]\n",
    "    for k in range(top):\n",
    "        if pred[k][1] not in DogBreed:\n",
    "            continue\n",
    "        labl = DogBreed[pred[k][1]]\n",
    "        if labl == gen_labl(idx):\n",
    "            true += 1\n",
    "            break\n",
    "\n",
    "\n",
    "print(true)\n",
    "print(true / ntest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ce08ddae0a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "img_path = '../../Stanford_Dog_Breed/Images/n02085620-Chihuahua/n02085620_10074.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "features = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 512)\n",
      "[('n02085620', 'Chihuahua', 0.99632937), ('n02087046', 'toy_terrier', 0.00053993583), ('n02112018', 'Pomeranian', 0.00050796004)]\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from an arbitrary intermediate layer with VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
      "574717952/574710816 [==============================] - 14s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "base_model = VGG19(weights='imagenet')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n",
    "\n",
    "img_path = '../../Stanford_Dog_Breed/Images/n02085620-Chihuahua/n02085620_10074.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "block4_pool_features = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14, 14, 512)\n"
     ]
    }
   ],
   "source": [
    "print(block4_pool_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune InceptionV3 on a new set of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(200, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
